[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Best Practices for Using eBird Data",
    "section": "",
    "text": "Matthew Strimas-Mackey, Wesley M. Hochachka, Viviana Ruiz-Gutierrez, Orin J. Robinson, Eliot T. Miller, Tom Auer, Steve Kelling, Daniel Fink, Alison Johnston\nVersion 2.0\nThis is a development version of Best Practices for Using eBird Data v2. It is a work in progress and should be treated as experimental.\n\nWelcome\nBest Practices for Using eBird Data is a supplement to Analytical guidelines to increase the value of community science data: An example using eBird data to estimate species distributions (Johnston et al. 2021). This paper describes the challenges associated with making inferences from biological citizen science data and proposes a set of best practices for making reliable estimates of species distributions from these data. Throughout, the paper uses eBird, the world’s largest biological citizen science project, as a case study to illustrate the best practices. This book acts as a supplement to the paper, showing readers how to implement these best practices within R using real data from eBird. After completing this book, readers should be able to process eBird data to prepare them for robust analyses, train models to estimate encounter rate, occupancy, and relative abundance, and assess the performance of these models . Readers should be comfortable with the R programming language, and read the Prerequisites and Setup sections of the introduction, before diving into this book.\nTo submit fixes or suggest additions and improvements to this book, please file an issue on GitHub.\nPlease cite this book as:\n\nStrimas-Mackey, M., W.M. Hochachka, V. Ruiz-Gutierrez, O.J. Robinson, E.T. Miller, T. Auer, S. Kelling, D. Fink, A. Johnston. 2023. Best Practices for Using eBird Data. Version 2.0. https://ebird.github.io/ebird-best-practices/. Cornell Lab of Ornithology, Ithaca, New York. https://doi.org/10.5281/zenodo.3620739"
  },
  {
    "objectID": "intro.html#intro-intro",
    "href": "intro.html#intro-intro",
    "title": "1  Introduction and Setup",
    "section": "1.1 Introduction",
    "text": "1.1 Introduction\nCitizen science data are increasingly making important contributions to ecological research and conservation. One of the most common forms of citizen science data is derived from members of the public recording species observations. eBird (Sullivan et al. 2014) is the largest of these biological citizen science programs. The eBird database contains well over one billion bird observations from every country in the world, with observations of nearly every bird species on Earth. The eBird database is valuable to researchers across the globe, due to its year-round, broad spatial coverage, high volumes of open access data, and applications to many ecological questions. These data have been widely used in scientific research to study phenology, species distributions, population trends, evolution, behavior, global change, and conservation. However, robust inference with eBird data requires careful processing of the data to address the challenges associated with citizen science datasets. This book, and the associated paper, outlines a set of best practices for addressing these challenges and making reliable estimates of species distributions from eBird data.\nThere are two key characteristics that distinguish eBird from many other citizen science projects and facilitate robust ecological analyses: the checklist structure enables non-detection to be inferred and the effort information associated with a checklist facilitates robust analyses by accounting for variation in the observation process (La Sorte et al. 2018; Kelling et al. 2018). When a participant submits data to eBird, sightings of multiple species from the same observation period are grouped together into a single checklist. Complete checklists are those for which the participant reported all birds that they were able to detect and identify. Critically, this enables scientists to infer counts of zero individuals for the species that were not reported. If checklists are not complete, it’s not possible to ascertain whether the absence of a species on a list was a non-detection or the result of a participant not recording the species. In addition, citizen science projects occur on a spectrum from those with predefined sampling structures that resemble more traditional survey designs, to those that are unstructured and collect observations opportunistically. eBird is a semi-structured project, having flexible, easy to follow protocols that attract many participants, but also collecting data on the observation process (e.g. amount of time spent birding, number of observers, etc.), which can be used in subsequent analyses (Kelling et al. 2018).\nDespite the strengths of eBird data, species observations collected through citizen science projects present a number of challenges that are not found in conventional scientific data. The following are some of the primary challenges associated these data; challenges that will be addressed throughout this book:\n\nTaxonomic bias: participants often have preferences for certain species, which may lead to preferential recording of some species over others (Greenwood 2007; Tulloch and Szabo 2012). Restricting analyses to complete checklists largely mitigates this issue.\nSpatial bias: most participants in citizen science surveys sample near their homes (Luck et al. 2004), in easily accessible areas such as roadsides (Kadmon, Farber, and Danin 2004), or in areas and habitats of known high biodiversity (Prendergast et al. 1993). A simple method to reduce the spatial bias that we describe is to create an equal area grid over the region of interest, and sample a given number of checklists from within each grid cell.\nTemporal bias: participants preferentially sample when they are available, such as weekends (Courter et al. 2013), and at times of year when they expect to observe more birds, notably during spring migration (Sullivan et al. 2014). To address the weekend bias, we recommend using a temporal scale of a week or multiple weeks for most analyses.\nSpatial precision: the spatial location of an eBird checklist is given as a single latitude-longitude point; however, this may not be precise for two main reasons. First, for traveling checklists, this location represents just one point on the journey. Second, eBird checklists are often assigned to a hotspot (a common location for all birders visiting a popular birding site) rather than their true location. For these reasons, it’s not appropriate to align the eBird locations with very precise habitat covariates, and we recommend summarizing covariates within a neighborhood around the checklist location.\nClass imbalance: bird species that are rare or hard to detect may have data with high class imbalance, with many more checklists with non-detections than detections. For these species, a distribution model predicting that the species is absent everywhere will have high accuracy, but no ecological value. We’ll follow the methods for addressing class imbalance proposed by Robinson et al. (2018).\nVariation in detectability: detectability describes the probability of a species that is present in an area being detected and identified. Detectability varies by season, habitat, and species (Johnston et al. 2014, 2018). Furthermore, eBird data are collected with high variation in effort, time of day, number of observers, and external conditions such as weather, all of which can affect the detectability of species (Ellis and Taylor 2018; Oliveira et al. 2018). Therefore, detectability is particularly important to consider when comparing between seasons, habitats or species. Since eBird uses a semi-structured protocol, that collects variables associated with variation in detectability, we’ll be able to account for a larger proportion of this variation in our analyses.\n\nThe remainder of this book will demonstrate how to address these challenges using real data from eBird to produce reliable estimates of species distributions. In general, we’ll take a two-pronged approach to dealing with unstructured data and maximizing the value of citizen science data: imposing more structure onto the data via data filtering and including covariates in models to account for the remaining variation.\nThe next chapter demonstrates how to access and prepare eBird data for modeling. The following chapter covers preparing environmental variables to be used as model predictors. The remaining three chapters provide examples of different species distribution models that can be fit using these data: encounter rate models, relative abundance models, and occupancy models. Although these examples focus on the use of eBird data, in many cases the techniques they illustrate also apply to similar citizen science datasets."
  },
  {
    "objectID": "intro.html#intro-pre",
    "href": "intro.html#intro-pre",
    "title": "1  Introduction and Setup",
    "section": "1.2 Prerequisites",
    "text": "1.2 Prerequisites\nTo understand the code examples used throughout this book, some knowledge of the programming language R is required. If you don’t meet this requirement, or begin to feel lost trying to understand the code used in this book, we suggest consulting one of the excellent free resources available online for learning R. For those with little or no prior programming experience, Hands-On Programming with R is an excellent introduction. For those with some familiarity with the basics of R that want to take their skills to the next level, we suggest R for Data Science as the best resource for learning how to work with data within R.\n\n1.2.1 Tidyverse\nThroughout this book, we use packages from the Tidyverse, an opinionated collection of R packages designed for data science. Packages such as ggplot2, for data visualization, and dplyr, for data manipulation, are two of the most well known Tidyverse packages; however, there are many more. In the following chapters, we often use Tidyverse functions without explanation. If you encounter a function you’re unfamiliar with, consult the documentation for help (e.g. ?mutate to see help for the dplyr function mutate()). More generally, the free online book R for Data Science by Hadley Wickham is the best introduction to working with data in R using the Tidyverse.\nThe one piece of the Tidyverse that we will cover here, because it is ubiquitous throughout this book and unfamiliar to many, is the pipe operator %>%. The pipe operator takes the expression to the left of it and “pipes” it into the first argument of the expression on the right, i.e. one can replace f(x) with x %>% f(). The pipe makes code significantly more readable by avoiding nested function calls, reducing the need for intermediate variables, and making sequential operations read left-to-right. For example, to add a new variable to a data frame, then summarize using a grouping variable, the following are equivalent:\n\nlibrary(dplyr)\n\n# pipes\nmtcars %>% \n  mutate(wt_kg = 454 * wt) %>% \n  group_by(cyl) %>% \n  summarize(wt_kg = mean(wt_kg))\n#> # A tibble: 3 × 2\n#>     cyl wt_kg\n#>   <dbl> <dbl>\n#> 1     4 1038.\n#> 2     6 1415.\n#> 3     8 1816.\n\n# intermediate variables\nmtcars_kg <- mutate(mtcars, wt_kg = 454 * wt)\nmtcars_grouped <- group_by(mtcars_kg, cyl)\nsummarize(mtcars_grouped, wt_kg = mean(wt_kg))\n#> # A tibble: 3 × 2\n#>     cyl wt_kg\n#>   <dbl> <dbl>\n#> 1     4 1038.\n#> 2     6 1415.\n#> 3     8 1816.\n\n# nested function calls\nsummarize(\n  group_by(\n    mutate(mtcars, wt_kg = 454 * wt),\n    cyl\n  ),\n  wt_kg = mean(wt_kg)\n)\n#> # A tibble: 3 × 2\n#>     cyl wt_kg\n#>   <dbl> <dbl>\n#> 1     4 1038.\n#> 2     6 1415.\n#> 3     8 1816.\n\nOnce you become familiar with the pipe operator, we believe you’ll find the the above example using the pipe the easiest of the three to read and interpret."
  },
  {
    "objectID": "intro.html#intro-setup",
    "href": "intro.html#intro-setup",
    "title": "1  Introduction and Setup",
    "section": "1.3 Setup",
    "text": "1.3 Setup\n\n1.3.1 Data package\nThe next two chapters of this book focus on obtaining and preparing eBird data and environmental variables for the modeling that will occur in the remaining chapters. These steps can be time consuming and laborious. If you’d like to skip straight to the analysis, download this package of prepared data. Unzip this file so that the contents are in the data/ subdirectory of your RStudio project folder. This will allow you to jump right in to the modeling and ensure that you’re using exactly the same data as was used when creating this book. This is a good option if you don’t have a fast enough internet connection to download the eBird data.\n\n\n1.3.2 Software\nThe examples throughout this website use the programming language R (R Core Team 2018) to work with eBird data. If you don’t have R installed, download it now, if you already have R, there’s a good chance you have an outdated version, so update it to the latest version now. R is updated regularly, and it is important that you have the most recent version of R to avoid headaches when installing packages. We suggest checking every couple months to see if a new version has been released.\nWe strongly encourage R users to use RStudio. RStudio is not required to follow along with this book; however, it will make your R experience significantly better. If you don’t have RStudio, download it now, if you already have it, update it because new versions with useful additional features are regularly released.\nDue to the large size of the eBird dataset, working with it requires the Unix command-line utility AWK. You won’t need to use AWK directly, since the R package auk does this hard work for you, but you do need AWK to be installed on your computer. Linux and Mac users should already have AWK installed on their machines; however, Windows user will need to install Cygwin to gain access to AWK. Cygwin is free software that allows Windows users to use Unix tools. Cygwin should be installed in the default location (C:/cygwin/bin/gawk.exe or C:/cygwin64/bin/gawk.exe) in order for everything to work correctly. Note: there’s no need to do anything at the “Select Utilities” screen, AWK will be installed by default.\n\n\n1.3.3 R packages\nThe examples in this book use a variety of R packages for accessing eBird data, working with spatial data, data processing and manipulation, and model fitting. To install all the packages necessary to work through this book, run the following code:\n\nif (!requireNamespace(\"pak\", quietly = TRUE)) {\n  install.packages(\"pak\")\n}\npak::pak(\"ebird/ebird-best-practices\")\n\nNote that several of the spatial packages require dependencies. If installing these packages fails, consult the instructions for installing dependencies on the sf package website. Finally, ensure all R packages are updated to their most recent version by clicking on the Update button on the Packages tab in RStudio.\n\n\n1.3.4 eBird data access\nAccess to the eBird database is provided via the eBird Basic Dataset (EBD) as tab-separated text files. To access the EBD, begin by creating an eBird account and signing in. Then visit the eBird Data Access page and fill out the data access request form. eBird data access is free for most uses; however, you will need to request access in order to download the EBD. Filling out the access request form allows eBird to keep track of the number of people using the data and obtain information on the applications for which the data are used.\nOnce you’ve granted access to the EBD, you will be able to download either the entire eBird dataset or subsets for specific species, regions, or time periods. This is covered in more detail in the next chapter.\nTo use eBird data in R, you’ll need to reference the full path to the text files, for example ~/data/ebird/ebd_relMarch-2023.txt. In general, it’s best to avoid using absolute paths in R scripts because it makes them less portable–if you’re sharing the files with someone else, they’ll need to change the file paths to point to the location where they’ve stored the eBird data. The R package auk provides a workaround for this, by allowing users to set an environment variable (EBD_PATH) that points to the directory where you’ve stored the eBird data. We suggest choosing a sensible central location for all eBird data files such as ~/data/ebird/. Then set the environment variable using auk_set_ebd_path().\n\n# set EBD_PATH environment variable\nauk::auk_set_ebd_path(\"~/data/ebird/\")\n\n\n\n1.3.5 GIS data\nThroughout this book, we’ll be producing maps of species distributions. To provide context for these distributions, we’ll need GIS data for political boundaries. Natural Earth is the best source for a range of tightly integrated vector and raster GIS data for producing professional cartographic maps. The R package, rnaturalearth provides a convenient method for accessing these data from within R.\nIn the next chapter, we provide a link to download a data package that will include all the necessary GIS data. However, for reference, the following code was used to generate the GIS dataset. Running this code will create a GeoPackage containing the necessary spatial layers in data/gis-data.gpkg.\n\nlibrary(dplyr)\nlibrary(rnaturalearth)\nlibrary(sf)\n\n# file to save spatial data\ngpkg_file <- \"data/gis-data.gpkg\"\ndir.create(dirname(gpkg_file), showWarnings = FALSE, recursive = TRUE)\n\n# political boundaries\n# land border with lakes removed\nne_land <- ne_download(scale = 50, category = \"cultural\",\n                       type = \"admin_0_countries_lakes\",\n                       returnclass = \"sf\") %>%\n  filter(CONTINENT == \"North America\") %>%\n  st_set_precision(1e6) %>%\n  st_union()\n# state boundaries for united states\nne_states <- ne_download(scale = 50, category = \"cultural\",\n                       type = \"admin_1_states_provinces\",\n                       returnclass = \"sf\") %>% \n  filter(iso_a2 == \"US\") %>% \n  select(state = name, state_code = iso_3166_2)\n# country lines\n# downloaded globally then filtered to north america with st_intersect()\nne_country_lines <- ne_download(scale = 50, category = \"cultural\",\n                                type = \"admin_0_boundary_lines_land\",\n                                returnclass = \"sf\") %>% \n  st_geometry()\nne_country_lines <- st_intersects(ne_country_lines, ne_land, sparse = FALSE) %>%\n  as.logical() %>%\n  {ne_country_lines[.]}\n# states, north america\nne_state_lines <- ne_download(scale = 50, category = \"cultural\",\n                              type = \"admin_1_states_provinces_lines\",\n                              returnclass = \"sf\") %>%\n  filter(ADM0_A3 %in% c(\"USA\", \"CAN\")) %>%\n  mutate(iso_a2 = recode(ADM0_A3, USA = \"US\", CAN = \"CAN\")) %>% \n  select(country = ADM0_NAME, country_code = iso_a2)\n\n# save all layers to a geopackage\nunlink(gpkg_file)\nwrite_sf(ne_land, gpkg_file, \"ne_land\")\nwrite_sf(ne_states, gpkg_file, \"ne_states\")\nwrite_sf(ne_country_lines, gpkg_file, \"ne_country_lines\")\nwrite_sf(ne_state_lines, gpkg_file, \"ne_state_lines\")\n\n\n\n\n\nCourter, Jason R., Ron J. Johnson, Claire M. Stuyck, Brian A. Lang, and Evan W. Kaiser. 2013. “Weekend Bias in Citizen Science Data Reporting: Implications for Phenology Studies.” International Journal of Biometeorology 57 (5): 715–20. https://doi.org/10.1007/s00484-012-0598-7.\n\n\nEllis, Murray V., and Jennifer E. Taylor. 2018. “Effects of Weather, Time of Day, and Survey Effort on Estimates of Species Richness in Temperate Woodlands.” Emu-Austral Ornithology 118 (2): 183–92.\n\n\nGreenwood, Jeremy J. D. 2007. “Citizens, Science and Bird Conservation.” Journal of Ornithology 148 (1): 77–124. https://doi.org/10.1007/s10336-007-0239-9.\n\n\nJohnston, Alison, Daniel Fink, Wesley M. Hochachka, and Steve Kelling. 2018. “Estimates of Observer Expertise Improve Species Distributions from Citizen Science Data.” Methods in Ecology and Evolution 9 (1): 88–97.\n\n\nJohnston, Alison, Stuart E. Newson, Kate Risely, Andy J. Musgrove, Dario Massimino, Stephen R. Baillie, and James W. Pearce-Higgins. 2014. “Species Traits Explain Variation in Detectability of UK Birds.” Bird Study 61 (3): 340–50.\n\n\nKadmon, Ronen, Oren Farber, and Avinoam Danin. 2004. “Effect of Roadside Bias on the Accuracy of Predictive Maps Produced by Bioclimatic Models.” Ecological Applications 14 (2): 401–13.\n\n\nKelling, Steve, Alison Johnston, Daniel Fink, Viviana Ruiz-Gutierrez, Rick Bonney, Aletta Bonn, Miguel Fernandez, et al. 2018. “Finding the Signal in the Noise of Citizen Science Observations.” bioRxiv, May, 326314. https://doi.org/10.1101/326314.\n\n\nLa Sorte, Frank A., Christopher A. Lepczyk, Jessica L. Burnett, Allen H. Hurlbert, Morgan W. Tingley, and Benjamin Zuckerberg. 2018. “Opportunities and Challenges for Big Data Ornithology.” The Condor 120 (2): 414–26.\n\n\nLuck, Gary W., Taylor H. Ricketts, Gretchen C. Daily, and Marc Imhoff. 2004. “Alleviating Spatial Conflict Between People and Biodiversity.” Proceedings of the National Academy of Sciences 101 (1): 182–86. https://doi.org/10.1073/pnas.2237148100.\n\n\nOliveira, Camilo Viana, Fabio Olmos, Manoel dos Santos-Filho, and Christine Steiner São Bernardo. 2018. “Observation of Diurnal Soaring Raptors In Northeastern Brazil Depends On Weather Conditions and Time of Day.” Journal of Raptor Research 52 (1): 56–65.\n\n\nPrendergast, J. R., S. N. Wood, J. H. Lawton, and B. C. Eversham. 1993. “Correcting for Variation in Recording Effort in Analyses of Diversity Hotspots.” Biodiversity Letters, 39–53.\n\n\nR Core Team. 2018. R: A Language and Environment for Statistical Computing. Vienna, Austria: R Foundation for Statistical Computing. https://www.R-project.org/.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, Daniel Fink, Robert J. Meese, Marcel Holyoak, and Evan G. Cooch. 2018. “Using Citizen Science Data in Integrated Population Models to Inform Conservation Decision-Making.” bioRxiv, 293464.\n\n\nSullivan, Brian L., Jocelyn L. Aycrigg, Jessie H. Barry, Rick E. Bonney, Nicholas Bruns, Caren B. Cooper, Theo Damoulas, et al. 2014. “The eBird Enterprise: An Integrated Approach to Development and Application of Citizen Science.” Biological Conservation 169 (January): 31–40. https://doi.org/10.1016/j.biocon.2013.11.003.\n\n\nTulloch, Ayesha IT, and Judit K. Szabo. 2012. “A Behavioural Ecology Approach to Understand Volunteer Surveying for Citizen Science Datasets.” Emu-Austral Ornithology 112 (4): 313–25."
  },
  {
    "objectID": "ebird.html#ebird-intro",
    "href": "ebird.html#ebird-intro",
    "title": "2  eBird Data",
    "section": "2.1 Introduction",
    "text": "2.1 Introduction\neBird data are collected and organized around the concept of a checklist, representing observations from a single birding event, such as a 1 km walk through a park or 15 minutes observing bird feeders in your backyard. Each checklist contains a list of species observed, counts of the number of individuals seen of each species, the location and time of the observations, and a measure of the effort expended while collecting these data. The following image depicts a typical eBird checklist as viewed on the eBird website:\n\nAlthough eBird collects semi-structured citizen science data, three elements of eBird checklists distinguish them from data collected via most other similar citizen science projects. First, eBird checklist require users to specify the survey protocol they used, whether it’s traveling, stationary, incidental (i.e. if the observations were collected when birding was not the primary activity), or one of the other protocols. Second, in addition to typical information on when and where the data were collected, checklists contain effort information specifying how long the observer searched, how far they traveled, and how many observers were part of the party. Finally, observers are asked to indicate whether they are reporting all the birds they were able to identify. Checklists with all species reported, known as complete checklists, enable researchers to identify which species were not detected (rather than just not reported). These inferred non-detections allow data to be zero-filled, so there’s a zero count for any species not recorded. Complete checklists with effort information facilitate robust analyses, and thus represent the gold standard of eBird checklists. Because of these factors, eBird data are often referred to as semi-structured (Kelling et al. 2018).\neBird data are typically distributed in two parts: observation data and checklist data. In the observation dataset, each row corresponds to the sighting of a single species on a checklist, including the count and any other species-level information (e.g. age, sex, species comments, etc.). In the checklist dataset, each row corresponds to a checklist, including the date, time, location, effort (e.g. distance traveled, time spent, etc.), and any additional checklist-level information (e.g. whether this is a complete checklist or not). The two datasets can be joined together using a unique checklist identifier (sometimes referred to as the sampling event identifier).\nIn this chapter, we’ll download eBird data and demonstrate how to use the R package auk to extract subsets of the data for analysis. Next, we’ll show how to import the data into R and zero-fill it to produce detection/non-detection data suitable for modeling species distribution and abundance. Finally, we’ll perform some pre-processing steps required to ensure proper analysis of the data."
  },
  {
    "objectID": "ebird.html#ebird-download",
    "href": "ebird.html#ebird-download",
    "title": "2  eBird Data",
    "section": "2.2 Downloading data",
    "text": "2.2 Downloading data\nThe observation and checklist data are released as tab-separated text files referred to as the eBird Basic Dataset (EBD) and the Sampling Event Data (SED), respectively. These files are released monthly and contain all validated bird sightings in the eBird database at the time of release. The EBD (i.e. the observation data) can be downloaded in its entirety or a subset for a given species, region, or time period can be requested via the “Custom Download” form. We strongly recommend against attempting to download the complete EBD since it’s well over 100GB at the time of writing. Instead, we will demonstrate a workflow using the “Custom Download” approach. In what follows, we will assume you have followed the instructions for requesting access to eBird data outlined in the previous chapter.\nIn the interest of making examples concrete, throughout this book, we’ll use the specific example of Wood Thrush observations from Georgia (the US state, not the country) in June for our analyses. We’ll start by downloading the corresponding eBird observations by visiting the eBird Basic Dataset download page and filling out the Custom Download form to request eBird data from Georgia.\n\nOnce the data are ready, you will an email with a download link. The downloaded data will be in a compressed .zip format, and should be unarchived. The resulting directory will contain a text file (e.g. ebd_US-GA_woothr_relMar-2023.txt) containing all the Wood Thrush observations from Georgia. The relMar-2023 component of the file name describes which version of the EBD this dataset came from; in this case it’s the March 2023 release.\nFor some uses cases, such as making a map of eBird observations, the EBD will be sufficient. However, for many applications, including all analyses in this book, we need both the detections and non-detections (i.e. the checklists where the species was not detected). To infer the non-detections we’ll need the full population of checklists for our region and time period of interest, which is provided in the form of the SED. Unlike the EBD, the SED can only be downloaded in it’s entirety rather than as a custom subset; however, the file is much smaller (~4GB at the time of writing) and therefore easier to deal with.\nDownload the Sampling Event Data now, then use a compression utility to uncompress the .tar file and the .txt.gz file it contains to produce a text file (e.g. ebd_sampling_relMar-2023.txt). It’s absolutely critical to confirm that the versions of the EBD and SED are the same, for example both files have relMar-2023 in the name, otherwise they will be mismatched when we later try to combine them. Move these the EBD and SED text files to the central location for eBird data that you set in the Introduction. To be reminded of the path to this directory use auk::auk_get_ebd_path()."
  },
  {
    "objectID": "ebird.html#ebird-filter",
    "href": "ebird.html#ebird-filter",
    "title": "2  eBird Data",
    "section": "2.3 Data extraction with auk",
    "text": "2.3 Data extraction with auk\neBird contains an impressive amount of data (over 1 billion bird observations!); however, this makes the data particularly challenging to work with. Very large text files can’t be opened in R, Excel, or most other software because they may require more memory than your computer has access to. In addition, even if these files can be read into R, they can be extremely slow to process. Fortunately, the R package auk has been specifically designed to extract subsets of data from the EBD and SED for analysis using the Unix command line text processing utility AWK. AWK only allows for coarse filtering, but the data can always be further refined once in R. With this in mind, the goal when using auk should always be to subset the EBD and SED text files down to a manageable size, small enough that they can be imported into R for further processing or analysis. In our case, that will mean extracting Wood Thrush records from Georgia in June.\nFiltering the eBird data using auk requires three steps. First, reference the EBD and SED text files using the function auk_ebd(). If you’ve followed the setup instruction in the Introduction and the previous section, you’ll have these text files on your computer and will have pointed auk to the directory they’re stored in using auk_set_ebd_path().\nBefore running any of the following code, create an RStudio project for following along with this book. The project will be a self contained space for all the code, input data, and outputs that comprise the lessons in this book. In addition, using a project will ensure your working directory is set to the project directory.\nDownloading and extracting eBird data in this section can be time consuming and laborious. If you would prefer to skip to the next section, download the data package mentioned in the Introduction and jump to the {next section}(#ebird-zf). Just make sure you load the packages in the first code chunk before proceeding.\n\nlibrary(auk)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(gridExtra)\nlibrary(lubridate)\nlibrary(readr)\nlibrary(sf)\n\n# setup data directory\ndir.create(\"data\", showWarnings = FALSE, recursive = TRUE)\n\nebd <- auk_ebd(\"ebd_US-GA_woothr_relMar-2023.txt\", \n               file_sampling = \"ebd_sampling_relMar-2023.txt\")\n\nNext, define the filters that you want to apply to the EBD. Each field that you can filter on has an associated function. For example, we’ll filter to observations from Georgia with auk_state(), in June of any year with auk_date(), restrict observations to those from either Stationary or Traveling protocols with auk_protocol(), and only keep complete checklists with auk_complete() since we intend to zero-fill the data. For a full list of possible filters, consult the package documentation.\nSome of the filtering work has already been done for us by downloading an EBD subset via the Custom Download form: we only have observations for Wood Thrush in Georgia. However, it’s critical that we filter the EBD and SED in exactly the same way to produce exactly the same population of checklists, so we include the filter for Georgia even though it’s redundant for the EBD.\n\nebd_filters <- ebd %>% \n  # georgia, make sure to use the ebird region code here\n  auk_state(state = \"US-GA\") %>% \n  # june, use * to get data from any year\n  auk_date(date = c(\"*-06-01\", \"*-06-30\")) %>% \n  # restrict to the standard traveling and stationary count protocols\n  auk_protocol(protocol = c(\"Stationary\", \"Traveling\")) %>%\n  # only complete checklists to facilitate zero-filling\n  auk_complete()\nebd_filters\n#> Input \n#>   EBD: /Users/mes335/data/ebird/ebd_US-GA_woothr_relMar-2023.txt \n#>   Sampling events: /Users/mes335/data/ebird/ebd_sampling_relMar-2023.txt \n#> \n#> Output \n#>   Filters not executed\n#> \n#> Filters \n#>   Species: all\n#>   Countries: all\n#>   States: US-GA\n#>   Counties: all\n#>   BCRs: all\n#>   Bounding box: full extent\n#>   Years: all\n#>   Date: *-06-01 - *-06-30\n#>   Start time: all\n#>   Last edited date: all\n#>   Protocol: Stationary, Traveling\n#>   Project code: all\n#>   Duration: all\n#>   Distance travelled: all\n#>   Records with breeding codes only: no\n#>   Complete checklists only: yes\n\nPrinting the object ebd_filters above shows what filters have been set. At this point, we’ve only defined the filters, not applied them to the data. The last step is to use auk_filter() to compile the filters into an AWK script and run it to produce two output files: one for the EBD and one for the SED. This step typically takes at least 30 minutes to run since the SED file is so large. As a result, it’s wise to wrap this in an if statement, so it’s only run once. As noted in the Introduction, Windows users will need to install Cygwin for this next step to work.\n\n# output files\nf_ebd <- \"data/observations_woothr_june_us-ga.txt\"\nf_sed <- \"data/checklists_june_us-ga.txt\"\n\n# only run if the output files don't already exist\nif (!file.exists(f_ebd)|| !file.exists(f_sed)) {\n  auk_filter(ebd_filters, file = f_ebd, file_sampling = f_sed)\n}\n\nThe resulting SED file is now about 13MB, compared to over 4GB for the original dataset, which means it can easily be read into R!"
  },
  {
    "objectID": "ebird.html#ebird-zf",
    "href": "ebird.html#ebird-zf",
    "title": "2  eBird Data",
    "section": "2.4 Importing and zero-filling",
    "text": "2.4 Importing and zero-filling\nThe previous step left us with two tab separated text files, one for the EBD (i.e. observation data) and one for the SED (i.e. checklist data). Next, we’ll use auk_zerofill() to read these two files into R and combine them together to produce zero-filled, detection/non-detection data (also called presence/absence data). To just read the EBD or SED, but not combine them, use read_ebd() or read_sampling(), respectively.\n\nzf <- auk_zerofill(f_ebd, f_sed, collapse = TRUE)\n\nWhen any of the read functions from auk are used, two important processing steps occur by default behind the scenes. 1. Taxonomic rollup: eBird observations can be made at levels below species (e.g. subspecies) or above species (e.g. a bird that was only identified as Duck sp.); however, for most uses we’ll want observations at the species level. auk_rollup() is applied by default when auk_zerofill() is used. It drops all observations not identifiable to a species and rolls up all observations reported below species to the species level. 2. Collapsing group checklist: eBird also allows for group checklists, those shared by multiple users. These checklists lead to duplication or near duplication of records within the dataset and the function auk_unique(), applied by default by auk_zerofill(), addresses this by only keeping one independent copy of each checklist.\nFinally, by default auk_zerofill() returns a compact representation of the data, consisting of a list of two data frames, one with checklist data and the other with observation data; the use of collapse = TRUE combines these into a single data frame, which will be easier to work with.\nBefore continuing, we’ll transform some of the variables to a more useful form for modelling. We convert time to a decimal value between 0 and 24, and we force the distance travelled to 0 for stationary checklists. Notably, eBirders have the option of entering an “X” rather than a count for a species, to indicate that the species was present, but they didn’t keep track of how many individuals were observed. During the modeling stage, we’ll want the observation_count variable stored as an integer and we’ll convert “X” to NA to allow for this.\n\n# function to convert time observation to hours since midnight\ntime_to_decimal <- function(x) {\n  x <- hms(x, quiet = TRUE)\n  hour(x) + minute(x) / 60 + second(x) / 3600\n}\n\n# clean up variables\nzf <- zf %>% \n  mutate(\n    # convert X to NA\n    observation_count = if_else(observation_count == \"X\", \n                                NA_character_, observation_count),\n    observation_count = as.integer(observation_count),\n    # effort_distance_km to 0 for non-travelling counts\n    effort_distance_km = if_else(protocol_type != \"Traveling\", \n                                 0, effort_distance_km),\n    # convert duration to hours\n    effort_hours = duration_minutes / 60,\n    # convert time to decimal hours since midnight\n    hours_of_day = time_to_decimal(time_observations_started),\n    # split date into year and day of year\n    year = year(observation_date),\n    day_of_year = yday(observation_date)\n  )"
  },
  {
    "objectID": "ebird.html#ebird-detect",
    "href": "ebird.html#ebird-detect",
    "title": "2  eBird Data",
    "section": "2.5 Accounting for variation in detectability",
    "text": "2.5 Accounting for variation in detectability\nAs discussed in the Introduction, variation in effort between checklists makes inference challenging, because it is associated with variation in detectability. When working with semi-structured datasets like eBird, one approach to dealing with this variation is to impose some more consistent structure on the data by filtering observations on the effort variables. This reduces the variation in detectability between checklists. Based on our experience working with these data, we suggest restricting checklists to less than 24 hours in duration and 10 km in length, and with 10 or fewer observers. Furthermore, we’ll only consider data from the past 10 years (2013-2022).\n\n# additional filtering\nzf_filtered <- zf %>% \n  filter(\n    # last 10 years of data\n    year >= 2013,\n    # effort filters\n    effort_hours <= 24,\n    effort_distance_km <= 10,\n    number_observers <= 10)\n\nFinally, there are a large number of variables in the EBD that are redundant (e.g. both state names and codes are present) or unnecessary for most modeling exercises (e.g. checklist comments and Important Bird Area codes). These can be removed at this point, keeping only the variables we want for modelling. Then we’ll save the resulting zero-filled observations for use in later chapters.\n\nchecklists <- zf_filtered %>% \n  select(checklist_id, observer_id, sampling_event_identifier,\n         scientific_name,\n         observation_count, species_observed, \n         state_code, locality_id, latitude, longitude,\n         protocol_type, all_species_reported,\n         observation_date, year, day_of_year,\n         hours_of_day, \n         effort_hours, effort_distance_km,\n         number_observers)\nwrite_csv(checklists, \"data/checklists-zf_woothr_june_us-ga.csv\", na = \"\")\n\nIf you’d like to ensure you’re using exactly the same data as was used to generate this book, download the data package mentioned in the setup instructions and place the contents in the data/ subdirectory of your project directory."
  },
  {
    "objectID": "ebird.html#ebird-explore",
    "href": "ebird.html#ebird-explore",
    "title": "2  eBird Data",
    "section": "2.6 Exploratory analysis and visualization",
    "text": "2.6 Exploratory analysis and visualization\nBefore proceeding to fitting species distribution models with these data, it’s worth exploring the dataset to see what we’re working with. Let’s start by making a simple map of the observations. This map uses GIS data available for download in the data package. Place the contents of the zip file in the data/ subdirectory of your project directory.\n\n# load and project gis data to albers equal area conic projection\nmap_proj <- st_crs(\"ESRI:102003\")\nne_land <- read_sf(\"data/gis-data.gpkg\", \"ne_land\") %>% \n  st_transform(crs = map_proj) %>% \n  st_geometry()\nne_country_lines <- read_sf(\"data/gis-data.gpkg\", \"ne_country_lines\") %>% \n  st_transform(crs = map_proj) %>% \n  st_geometry()\nne_state_lines <- read_sf(\"data/gis-data.gpkg\", \"ne_state_lines\") %>% \n  st_transform(crs = map_proj) %>% \n  st_geometry()\nga_boundary <- read_sf(\"data/gis-data.gpkg\", \"ne_states\") %>% \n  filter(state_code == \"US-GA\") %>% \n  st_transform(crs = map_proj) %>% \n  st_geometry()\n\n# prepare ebird data for mapping\nchecklists_sf <- checklists %>% \n  # convert to spatial points\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %>% \n  st_transform(crs = map_proj) %>% \n  select(species_observed)\n\n# map\npar(mar = c(0.25, 0.25, 0.25, 0.25))\n# set up plot area\nplot(st_geometry(checklists_sf), col = NA)\n# contextual gis data\nplot(ne_land, col = \"#dddddd\", border = \"#888888\", lwd = 0.5, add = TRUE)\nplot(ga_boundary, col = \"#cccccc\", border = NA, add = TRUE)\nplot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\nplot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\n# ebird observations\n# not observed\nplot(filter(checklists_sf, !species_observed),\n     pch = 19, cex = 0.1, col = alpha(\"#555555\", 0.25),\n     add = TRUE)\n# observed\nplot(filter(checklists_sf, species_observed),\n     pch = 19, cex = 0.3, col = alpha(\"#4daf4a\", 1),\n     add = TRUE)\n# legend\nlegend(\"bottomright\", bty = \"n\",\n       col = c(\"#555555\", \"#4daf4a\"),\n       legend = c(\"eBird checklists\", \"Wood Thrush sightings\"),\n       pch = 19)\nbox()\npar(new = TRUE, mar = c(0, 0, 3, 0))\ntitle(\"Wood Thrush eBird Observations\\nJune 2013-2022\")\n\n\n\n\n\n\n\n\nIn this map, the spatial bias in eBird data becomes immediately obvious, for example, notice the large number of checklists areas around Atlanta in the northern part of the state.\nExploring the effort variables is also a valuable exercise. For each effort variable, we’ll produce both a histogram and a plot of frequency of detection as a function of that effort variable. The histogram will tell us something about birder behavior. For example, what time of day are most people going birding, and for how long? We may also want to note values of the effort variable that have very few observations; predictions made in these regions may be unreliable due to a lack of data. The detection frequency plots tell us how the probability of detecting a species changes with effort.\n\n2.6.1 Time of day\nThe chance of an observer detecting a bird when present can be highly dependent on time of day. For example, many species exhibit a peak in detection early in the morning during dawn chorus and a secondary peak early in the evening. With this in mind, the first predictor of detection that we’ll explore is the time of day at which a checklist was started. We’ll summarize the data in 1 hour intervals, then plot them. Since estimates of detection frequency are unreliable when only a small number of checklists are available, we’ll only plot hours for which at least 100 checklists are present.\n\n# summarize data by hourly bins\nbreaks <- seq(0, 24)\nlabels <- breaks[-length(breaks)] + diff(breaks) / 2\nchecklists_time <- checklists %>% \n  mutate(hour_bins = cut(hours_of_day, \n                         breaks = breaks, \n                         labels = labels,\n                         include.lowest = TRUE),\n         hour_bins = as.numeric(as.character(hour_bins))) %>% \n  group_by(hour_bins) %>% \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_tod_hist <- ggplot(checklists_time) +\n  aes(x = hour_bins, y = n_checklists) +\n  geom_segment(aes(xend = hour_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 24, by = 3), limits = c(0, 24)) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Hours since midnight\",\n       y = \"# checklists\",\n       title = \"Distribution of observation start times\")\n\n# frequency of detection\ng_tod_freq <- ggplot(checklists_time %>% filter(n_checklists > 100)) +\n  aes(x = hour_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = seq(0, 24, by = 3), limits = c(0, 24)) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Hours since midnight\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_tod_hist, g_tod_freq)\n\n\n\n\n\n\n\n\nAs expected, Wood Thrush detectability is highest early in the morning and quickly falls off as the day progresses. In later chapters, we’ll make predictions at the peak time of day for detectability to limit the effect of this variation. The majority of checklist submissions also occurs in the morning; however, there are reasonable numbers of checklists between 6am and 9pm. It’s in this region that our model estimates will be most reliable.\n\n\n2.6.2 Checklist duration\nWhen we initially extracted the eBird data in Section @ref(ebird-extract), we restricted observations to those from checklists 24 hours in duration or shorter to reduce variability. Let’s see what sort of variation remains in checklist duration.\n\n# summarize data by hour long bins\nbreaks <- seq(0, 24)\nlabels <- breaks[-length(breaks)] + diff(breaks) / 2\nchecklists_duration <- checklists %>% \n  mutate(duration_bins = cut(effort_hours, \n                             breaks = breaks, \n                             labels = labels,\n                             include.lowest = TRUE),\n         duration_bins = as.numeric(as.character(duration_bins))) %>% \n  group_by(duration_bins) %>% \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_duration_hist <- ggplot(checklists_duration) +\n  aes(x = duration_bins, y = n_checklists) +\n  geom_segment(aes(xend = duration_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = 0:5) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Checklist duration (hours)\",\n       y = \"# checklists\",\n       title = \"Distribution of checklist durations\")\n\n# frequency of detection\ng_duration_freq <- ggplot(checklists_duration %>% filter(n_checklists > 100)) +\n  aes(x = duration_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 0:5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Checklist duration (hours)\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_duration_hist, g_duration_freq)\n\n\n\n\n\n\n\n\nThe majority of checklists are an hour or shorter and there is a rapid decline in the frequency of checklists with increasing duration. In addition, longer searches yield a higher chance of detecting a Wood Thrush. In many cases, there is a saturation effect, with searches beyond a given length producing little additional benefit; however, here there appears to be a drop off in detection for checklists longer than 3.5 hours.\n\n\n2.6.3 Distance traveled\nAs with checklist duration, we expect a priori that the greater the distance someone travels, the greater the probability of encountering at least one Wood Thrush. Let’s see if this expectation is met. Note that we have already truncated the data to checklists less than 10 km in length.\n\n# summarize data by 1 km bins\nbreaks <- seq(0, 10)\nlabels <- breaks[-length(breaks)] + diff(breaks) / 2\nchecklists_dist <- checklists %>% \n  mutate(dist_bins = cut(effort_distance_km, \n                         breaks = breaks, \n                         labels = labels,\n                         include.lowest = TRUE),\n         dist_bins = as.numeric(as.character(dist_bins))) %>% \n  group_by(dist_bins) %>% \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_dist_hist <- ggplot(checklists_dist) +\n  aes(x = dist_bins, y = n_checklists) +\n  geom_segment(aes(xend = dist_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = 0:5) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Distance travelled (km)\",\n       y = \"# checklists\",\n       title = \"Distribution of distance travelled\")\n\n# frequency of detection\ng_dist_freq <- ggplot(checklists_dist %>% filter(n_checklists > 100)) +\n  aes(x = dist_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = 0:5) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"Distance travelled (km)\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_dist_hist, g_dist_freq)\n\n\n\n\n\n\n\n\nAs with duration, the majority of observations are from short checklists (less than half a kilometer). One fortunate consequence of this is that most checklists will be contained within a small area within which habitat is not likely to show high variability. In chapter @ref{envvar}, we will summarize land cover data within circles 3 km in diameter, centered on each checklist, and it appears that the vast majority of checklists will stay contained within this area.\n\n\n2.6.4 Number of observers\nFinally, let’s consider the number of observers whose observation are being reported in each checklist. We expect that at least up to some number of observers, reporting rates will increase; however, in working with these data we have found cases of declining detection rates for very large groups. With this in mind we have already restricted checklists to those with 30 or fewer observers, thus removing the very largest groups (prior to filtering, some checklists had as many as 180 observers!).\n\n# summarize data\nbreaks <- seq(0, 10)\nlabels <- seq(1, 10)\nchecklists_obs <- checklists %>% \n  mutate(obs_bins = cut(number_observers, \n                        breaks = breaks, \n                        label = labels,\n                        include.lowest = TRUE),\n         obs_bins = as.numeric(as.character(obs_bins))) %>% \n  group_by(obs_bins) %>% \n  summarise(n_checklists = n(),\n            n_detected = sum(species_observed),\n            det_freq = mean(species_observed))\n\n# histogram\ng_obs_hist <- ggplot(checklists_obs) +\n  aes(x = obs_bins, y = n_checklists) +\n  geom_segment(aes(xend = obs_bins, y = 0, yend = n_checklists),\n               color = \"grey50\") +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"# observers\",\n       y = \"# checklists\",\n       title = \"Distribution of the number of observers\")\n\n# frequency of detection\ng_obs_freq <- ggplot(checklists_obs %>% filter(n_checklists > 100)) +\n  aes(x = obs_bins, y = det_freq) +\n  geom_line() +\n  geom_point() +\n  scale_x_continuous(breaks = breaks) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(x = \"# observers\",\n       y = \"% checklists with detections\",\n       title = \"Detection frequency\")\n\n# combine\ngrid.arrange(g_obs_hist, g_obs_freq)\n\n\n\n\n\n\n\n\nThe majority of checklists have one or two observers and there appears to be an increase in detection frequency with more observers. However, it’s hard to distinguish a discernible pattern in the noise here, likely because there are so few checklists with more than 3 observers.\n\n\n\n\nKelling, Steve, Alison Johnston, Daniel Fink, Viviana Ruiz-Gutierrez, Rick Bonney, Aletta Bonn, Miguel Fernandez, et al. 2018. “Finding the Signal in the Noise of Citizen Science Observations.” bioRxiv, May, 326314. https://doi.org/10.1101/326314."
  },
  {
    "objectID": "envvar.html#envvar-intro",
    "href": "envvar.html#envvar-intro",
    "title": "3  Environmental Variables",
    "section": "3.1 Introduction",
    "text": "3.1 Introduction\nSpecies distribution models work by finding associations between species occurrence or abundance and environmental variables. Using these relationships, it’s possible to predict the distribution in areas that aren’t sampled, provided we know the value of the environmental variables in these areas. Therefore, to proceed with the modeling in the next several chapters, we’ll need a suite of environmental variables to be used as predictors in our models. The particular set of environmental variables that’s most suitable for a given study will depend on the focal species, region, and time period, as well as the availability of data. When species distributions are well defined by the environmental variables, extrapolations to unsurveyed areas will be more accurate. So, it’s worth considering which variables are important for your species and region.\nFortunately, there are an abundance of freely available, satellite-based environmental datasets that are suitable for species distribution modeling. A small subset of possible data sources available globally includes data describing landcover, elevation, topography, surface water, and intertidal habitat. However, the reader is encouraged to search for datasets suitable for their region and species of interest.\nSince there are such a wide range of available environmental datasets, and the distribution mechanisms and formats for each are different and often changing, we will not cover the specifics of how to download and pre-processing satellite-derived data products. Instead, we have downloaded and prepared example landcover and elevation datasets and will demonstrate how environmental variables can be extracted from these datasets in the following sections. This will provide examples of assigning environmental variables based on both categorical (landcover) and continuous (elevation) raster data sets.\nTo gain access to the example raster datasets, download the data package for by following the instructions in the Introduction."
  },
  {
    "objectID": "envvar.html#envvar-landcover",
    "href": "envvar.html#envvar-landcover",
    "title": "3  Environmental Variables",
    "section": "3.2 Landcover",
    "text": "3.2 Landcover\nFor the examples in this book, we’ll use land cover variables derived from the MODIS MCD12Q1 v006 land cover product (Friedl and Sulla-Menashe 2015). This product has global coverage at 500m spatial resolution and annual temporal resolution from 2001-2021. These data are available for several different classification schemes. We’ll use the FAO Land Cover Classification System (LCCS1), which provides a globally accurate classification of land cover in our experience. This system classifies pixels into one of 16 different land cover classes:\n\n\n\n\n\n\n\n\n\n\nclass\nname\ndescription\n\n\n\n\n1\nBarren\nAt least of area 60% is non-vegetated barren (sand, rock, soil) or permanent snow/ice with less than 10% vegetation.\n\n\n2\nPermanent Snow and Ice\nAt least of area 60% is covered by snow and ice for at least 10 months of the year.\n\n\n3\nWater Bodies\nAt least 60% of area is covered by permanent water bodies.\n\n\n11\nEvergreen Needleleaf Forests\nDominated by evergreen conifer trees (>2m). Tree cover >60%.\n\n\n12\nEvergreen Broadleaf Forests\nDominated by evergreen broadleaf and palmate trees (>2m). Tree cover >60%.\n\n\n13\nDeciduous Needleleaf Forests\nDominated by deciduous needleleaf (larch) trees (>2m). Tree cover >60%.\n\n\n14\nDeciduous Broadleaf Forests\nDominated by deciduous broadleaf trees (>2m). Tree cover >60%.\n\n\n15\nMixed Broadleaf/Needleleaf Forests\nCo-dominated (40-60%) by broadleaf deciduous and evergreen needleleaf tree (>2m) types. Tree cover >60%.\n\n\n16\nMixed Broadleaf Evergreen/Deciduous Forests\nCo-dominated (40-60%) by broadleaf evergreen and deciduous tree (>2m) types. Tree cover >60%.\n\n\n21\nOpen Forests\nTree cover 30-60% (canopy >2m).\n\n\n22\nSparse Forests\nTree cover 10-30% (canopy >2m).\n\n\n31\nDense Herbaceous\nDominated by herbaceous annuals (<2m) at least 60% cover.\n\n\n32\nSparse Herbaceous\nDominated by herbaceous annuals (<2m) 10-60% cover.\n\n\n41\nDense Shrublands\nDominated by woody perennials (1-2m) >60% cover.\n\n\n42\nShrubland/Grassland Mosaics\nDominated by woody perennials (1-2m) 10-60% cover with dense herbaceous annual understory.\n\n\n43\nSparse Shrublands\nDominated by woody perennials (1-2m) 10-60% cover with minimal herbaceous understory.\n\n\n255\nUnclassified\nHas not received a map label because of missing inputs.\n\n\n\n\n\nThe 2013-2021 data for our focal region (i.e. Georgia) is the data package in the file data/modis_mcd12q1_fao_2013-2021.tif. This is multi-band GeoTIFF in which each band corresponds to a year of landcover data. In R, we’ll use the terra package package to work with raster datasets.\n\nlibrary(dplyr)\nlibrary(exactextractr)\nlibrary(landscapemetrics)\nlibrary(readr)\nlibrary(sf)\nlibrary(stringr)\nlibrary(terra)\nlibrary(tidyr)\nlibrary(units)\nlibrary(viridis)\n\n# load and inspect the landcover data\nlandcover <- rast(\"data/modis_mcd12q1_fao_2013-2021.tif\")\nprint(landcover)\n#> class       : SpatRaster \n#> dimensions  : 1173, 1249, 9  (nrow, ncol, nlyr)\n#> resolution  : 463, 463  (x, y)\n#> extent      : -8132528, -7553851, 3362724, 3906190  (xmin, xmax, ymin, ymax)\n#> coord. ref. : +proj=sinu +lon_0=0 +x_0=0 +y_0=0 +R=6371007.181 +units=m +no_defs \n#> source      : modis_mcd12q1_fao_2013-2021.tif \n#> names       : 2013, 2014, 2015, 2016, 2017, 2018, ... \n#> min values  :    1,    1,    1,    1,    1,    1, ... \n#> max values  :   43,   43,   43,   43,   43,   43, ...\n\n# map the data for 2021\nplot(as.factor(landcover[[\"2021\"]]), \n     main = \"MODIS Landcover 2021\",\n     axes = FALSE)\n\n\n\n\n\n\n\n\nAt this point we could use the MODIS land cover data directly, simply extracting the land cover class for each checklist location. However, we instead advocate summarizing the land cover data within a neighborhood around the checklist locations. As discussed in Section @ref(intro-intro), checklist locations are not precise, so it’s more appropriate to use the habitat in the surrounding area, rather than only at the checklist location. More fundamentally, organisms interact with their environment not at a single point, but at the scale of a landscape, so it’s important to include habitat information characterizing a suitably-sized landscape around the observation location. Based on our experience working with eBird data, a 3 km diameter circular neighborhood centered on each checklist location is sufficient to account for the spatial precision in the data when the maximum distance of travelling counts has been limited to 10 km, while also being a relevant ecological scale for many bird species.\nThere are a variety of landscape metrics that can be used to characterize the composition (what habitat is available) and configuration (how that habitat is arranged spatially) of landscapes. Many of these metrics can be calculated using the R package landscapemetrics. We’ll use two simple metrics to summarize landcover data in this book: percent landcover, a measure of composition, and edge density, a measure of configuration. For each landcover class, Percent landcover (abbreviated as pland) is the percent of the landscape that is composed of that class and edge density (abbreviated as ed) is the total boundary length of all patches of that class per unit area. For a broad range of scenarios, these two metrics are a reliable choice for calculating environmental covariates in distribution modeling.\nWe’ll start by finding the full set of unique checklists locations for each year in the eBird data and buffer the points by 1.5km to generate 3km diameter circular neighborhoods centered on each checklist location. Note that the landcover data are not available for 2022, so we use the 2021 landcover data for 2022 checklists.\n\n# ebird checklist locations\nchecklists <- read_csv(\"data/checklists-zf_woothr_june_us-ga.csv\") %>% \n  # landcover data not availble for the full period, so we use the closest year\n  mutate(year_lc = as.character(pmin(year, 2021)))\n\n# generate circular neighborhoods for all checkists\nbuffers <- checklists %>% \n  # identify unique location/year combinations\n  distinct(locality_id, year_lc, latitude, longitude) %>% \n  # generate a 3km neighborhoods\n  st_as_sf(coords = c(\"longitude\", \"latitude\"), crs = 4326) %>% \n  st_buffer(dist = set_units(1.5, \"km\"))\n\nNext, for each location, we crop and mask the landcover layer corresponding to the checklist year to the circular neighborhood around that checklist. Then we use calculate_lsm() from landscapemetrics to calculate pland and ed metrics within each neighborhood. This step may take 30 minute or longer to run.\n\nlsm <- NULL\nfor (i in seq_len(nrow(buffers))) {\n  buffer_i <- st_transform(buffers[i, ], crs = crs(landcover))\n  year <- as.character(buffer_i$year_lc)\n  \n  # crop and mask landcover raster so all values outside buffer are missing\n  lsm[[i]] <- crop(landcover[[year]], buffer_i) %>% \n    mask(buffer_i) %>% \n    # calcualte landscape metrics\n    calculate_lsm(level = \"class\", metric = c(\"pland\", \"ed\")) %>% \n    # add variables to uniquely identify each point\n    mutate(locality_id = buffer_i$locality_id, \n           year_lc = buffer_i$year_lc)\n}\nlsm <- bind_rows(lsm) %>% \n  select(locality_id, year_lc, class, metric, value)\n\nFinally, we’ll transform the data frame so there’s one row per location and all the environmental variables appear as columns. For each location, any landcover classes that don’t appear within the buffer will not have associated pland and ed metrics; at this stage, we replace these implicit missing values with zeros.\n\nlsm_wide <- lsm %>% \n  pivot_wider(values_from = value,\n              names_from = c(class, metric),\n              names_glue = \"landcover_c{str_pad(class, 2, pad = '0')}_{metric}\",\n              names_sort = TRUE,\n              values_fill = 0) %>% \n  arrange(locality_id, year_lc)"
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Courter, Jason R., Ron J. Johnson, Claire M. Stuyck, Brian A. Lang, and\nEvan W. Kaiser. 2013. “Weekend Bias in Citizen\nScience Data Reporting: Implications for Phenology\nStudies.” International Journal of Biometeorology 57\n(5): 715–20. https://doi.org/10.1007/s00484-012-0598-7.\n\n\nEllis, Murray V., and Jennifer E. Taylor. 2018. “Effects of\nWeather, Time of Day, and Survey Effort on Estimates of Species Richness\nin Temperate Woodlands.” Emu-Austral Ornithology 118\n(2): 183–92.\n\n\nFriedl, Mark, and Damien Sulla-Menashe. 2015. “MCD12Q1\nMODIS/Terra+Aqua Land Cover Type Yearly L3\nGlobal 500m SIN Grid V006.” NASA EOSDIS\nLand Processes DAAC. https://doi.org/10.5067/MODIS/MCD12Q1.006.\n\n\nGreenwood, Jeremy J. D. 2007. “Citizens, Science and Bird\nConservation.” Journal of Ornithology 148 (1): 77–124.\nhttps://doi.org/10.1007/s10336-007-0239-9.\n\n\nJohnston, Alison, Daniel Fink, Wesley M. Hochachka, and Steve Kelling.\n2018. “Estimates of Observer Expertise Improve Species\nDistributions from Citizen Science Data.” Methods in Ecology\nand Evolution 9 (1): 88–97.\n\n\nJohnston, Alison, Stuart E. Newson, Kate Risely, Andy J. Musgrove, Dario\nMassimino, Stephen R. Baillie, and James W. Pearce-Higgins. 2014.\n“Species Traits Explain Variation in Detectability of\nUK Birds.” Bird Study 61 (3): 340–50.\n\n\nKadmon, Ronen, Oren Farber, and Avinoam Danin. 2004. “Effect of\nRoadside Bias on the Accuracy of Predictive Maps Produced by Bioclimatic\nModels.” Ecological Applications 14 (2): 401–13.\n\n\nKelling, Steve, Alison Johnston, Daniel Fink, Viviana Ruiz-Gutierrez,\nRick Bonney, Aletta Bonn, Miguel Fernandez, et al. 2018. “Finding\nthe Signal in the Noise of Citizen Science\nObservations.” bioRxiv, May, 326314. https://doi.org/10.1101/326314.\n\n\nLa Sorte, Frank A., Christopher A. Lepczyk, Jessica L. Burnett, Allen H.\nHurlbert, Morgan W. Tingley, and Benjamin Zuckerberg. 2018.\n“Opportunities and Challenges for Big Data Ornithology.”\nThe Condor 120 (2): 414–26.\n\n\nLuck, Gary W., Taylor H. Ricketts, Gretchen C. Daily, and Marc Imhoff.\n2004. “Alleviating Spatial Conflict Between People and\nBiodiversity.” Proceedings of the National Academy of\nSciences 101 (1): 182–86. https://doi.org/10.1073/pnas.2237148100.\n\n\nOliveira, Camilo Viana, Fabio Olmos, Manoel dos Santos-Filho, and\nChristine Steiner São Bernardo. 2018. “Observation of\nDiurnal Soaring Raptors In Northeastern Brazil Depends On Weather\nConditions and Time of Day.”\nJournal of Raptor Research 52 (1): 56–65.\n\n\nPrendergast, J. R., S. N. Wood, J. H. Lawton, and B. C. Eversham. 1993.\n“Correcting for Variation in Recording Effort in Analyses of\nDiversity Hotspots.” Biodiversity Letters, 39–53.\n\n\nR Core Team. 2018. R: A Language and Environment for Statistical\nComputing. Vienna, Austria: R Foundation for Statistical Computing.\nhttps://www.R-project.org/.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, Daniel Fink, Robert J. Meese,\nMarcel Holyoak, and Evan G. Cooch. 2018. “Using Citizen Science\nData in Integrated Population Models to Inform Conservation\nDecision-Making.” bioRxiv, 293464.\n\n\nSullivan, Brian L., Jocelyn L. Aycrigg, Jessie H. Barry, Rick E. Bonney,\nNicholas Bruns, Caren B. Cooper, Theo Damoulas, et al. 2014. “The\neBird Enterprise: An Integrated\nApproach to Development and Application of Citizen Science.”\nBiological Conservation 169 (January): 31–40. https://doi.org/10.1016/j.biocon.2013.11.003.\n\n\nTulloch, Ayesha IT, and Judit K. Szabo. 2012. “A Behavioural\nEcology Approach to Understand Volunteer Surveying for Citizen Science\nDatasets.” Emu-Austral Ornithology 112 (4): 313–25."
  },
  {
    "objectID": "envvar.html",
    "href": "envvar.html",
    "title": "3  > [1] “NOT AVAILABLE”",
    "section": "",
    "text": "4 Environmental Variables"
  },
  {
    "objectID": "envvar.html#envvar-elevation",
    "href": "envvar.html#envvar-elevation",
    "title": "3  Environmental Variables",
    "section": "3.3 Elevation",
    "text": "3.3 Elevation\nIn this section we’ll demonstrate how to assign elevation to each checklist, which frequently plays an important role in shaping species distributions. Amatulli et al. (2018) provide a suite of global, 1km resolution topographic variables designed for use in distribution modeling. A range of variables are available, including elevation, slope, roughness, and many others; we’ll focus on elevation here, but the approach can easily be applied to other variables.\nTo access the data, visit the website for these data, download the 1 km resolution median elevation product, and save the file (elevation_1KMmd_GMTEDmd.tif) in the data/ subdirectory of your RStudio project. If you’re unable to download the data, we’ve also provided a small subset of the data covering our study extent in the data package.\nAnalogous to how we assigned landcover variables, we’ll calculate the mean and standard deviation of the elevation within 3km diameter circular neighborhoods centered on each checklist location.\n\n# elevation raster\nelevation <- rast(\"data/elevation_1KMmd_GMTEDmd.tif\")\n\n# mean and standard deviation within each circular neighborhood\nelev_buffer <- exact_extract(elevation, buffers, fun = c(\"mean\", \"stdev\"),\n                             progress = FALSE) %>% \n  # add variables to uniquely identify each point\n  mutate(locality_id = buffers$locality_id, year_lc = buffers$year_lc) %>% \n  select(locality_id, year_lc, \n         elevation_mean = mean,\n         elevation_sd = stdev)\n\nNow, let’s combine the landcover and elevation variables together, join them back to the checklist data, and save them as model predictors in the upcoming chapters.\n\n# combine elevation and landcover\nenv_variables <- inner_join(elev_buffer, lsm_wide,\n                            by = c(\"locality_id\", \"year_lc\"))\n\n# attach and expand to checklists\nenv_variables <- checklists %>% \n  select(checklist_id, locality_id, year_lc) %>% \n  inner_join(env_variables, by = c(\"locality_id\", \"year_lc\")) %>% \n  select(-locality_id, -year_lc)\n\n# save to csv\nwrite_csv(env_variables, \n          \"data/environmental-variables_checklists.csv\", \n          na = \"\")"
  },
  {
    "objectID": "envvar.html#envvar-pred",
    "href": "envvar.html#envvar-pred",
    "title": "3  Environmental Variables",
    "section": "3.4 Prediction surface",
    "text": "3.4 Prediction surface\nAfter training a species distribution model, the goal is typically to make predictions throughout the study area. To do this, we’ll need a prediction surface: a regular grid of habitat covariates over which to make predictions. In this section, we’ll create such a prediction surface for our study region (Georgia) using the MODIS land cover data from the most recent year for which they’re available in addition to elevation data. To start, we’ll create a template raster with cells equal in dimension to the diameter of the circular neighborhoods we used above. It’s important to use an equal area coordinate reference system for the prediction surface; we’ll use a Lambert’s azimuthal equal area projection centered on our study region.\n\n# lambert's azimuthal equal area projection for georgia\nlaea_crs <- st_crs(\"+proj=laea +lat_0=32.5 +lon_0=-83.5\")\n\n# study region: georgia\nstudy_region <- read_sf(\"data/gis-data.gpkg\", layer = \"ne_states\") %>% \n  filter(state_code == \"US-GA\") %>% \n  st_transform(crs = laea_crs)\n\n# create a raster template covering the region with 3km resolution\nr <- rast(study_region, res = c(3000, 3000))\n\n# fill the raster with 1s inside the study region\nr <- rasterize(study_region, r, values = 1) %>% \n  setNames(\"study_region\")\n\n# save for later use\nr <- writeRaster(r, \"data/prediction-surface.tif\",\n                 overwrite = TRUE,\n                 gdal = c(\"COMPRESS=DEFLATE\",\n                          \"TILED=YES\",\n                          \"COPY_SRC_OVERVIEWS=YES\"))\n\nNext, we extract the coordinates of the cell centers from the raster we just created, convert these to sf point features, and buffer these to generate 3km circular neighborhoods.\n\n# generate neighborhoods for the prediction surface cell centers\nbuffers_ps <- as.data.frame(r, cells = TRUE, xy = TRUE) %>% \n  select(cell_id = cell, x, y) %>% \n  st_as_sf(coords = c(\"x\", \"y\"), crs = laea_crs, remove = FALSE) %>% \n  st_transform(crs = 4326) %>% \n  st_buffer(set_units(3, \"km\"))\n\nNow we can calculate the landcover and elevation variables exactly as we did for the eBird checklists in the previous two sections. First, the landscape metrics pland and ed from the landcover data. Note that we use the most recent year of landcover data (i.e. 2021) in this case.\n\n# estimate landscape metrics for each cell in the prediction surface\nlsm_ps <- NULL\nfor (i in seq_len(nrow(buffers_ps))) {\n  buffer_i <- st_transform(buffers_ps[i, ], crs = crs(landcover))\n  \n  # crop and mask landcover raster so all values outside buffer are missing\n  lsm_ps[[i]] <- crop(landcover[[\"2021\"]], buffer_i) %>% \n    mask(buffer_i) %>% \n    # calcualte landscape metrics\n    calculate_lsm(level = \"class\", metric = c(\"pland\", \"ed\")) %>% \n    # add variable to uniquely identify each point\n    mutate(cell_id = buffer_i$cell_id)\n}\nlsm_ps <- bind_rows(lsm_ps) %>% \n  select(cell_id, class, metric, value)\n\n# transform to wide format\nlsm_wide_ps <- lsm_ps %>% \n  pivot_wider(values_from = value,\n              names_from = c(class, metric),\n              names_glue = \"landcover_c{str_pad(class, 2, pad = '0')}_{metric}\",\n              names_sort = TRUE,\n              values_fill = 0) %>% \n  arrange(cell_id)\n\nAnd now the mean and standard deviation of elevation.\n\nelev_buffer_ps <- exact_extract(elevation, buffers_ps, \n                                fun = c(\"mean\", \"stdev\"),\n                                progress = FALSE) %>% \n  # add variables to uniquely identify each point\n  mutate(cell_id = buffers_ps$cell_id) %>% \n  select(cell_id, elevation_mean = mean, elevation_sd = stdev)\n\nFinally, we combine the landcover and elevation variables together and save to CSV.\n\n# combine landcover and elevation\nenv_variables_ps <- inner_join(elev_buffer_ps, lsm_wide_ps, by = \"cell_id\")\n\n# attach the xy coordinates of the cell centers\nenv_variables_ps <- buffers_ps %>% \n  st_drop_geometry() %>% \n  select(cell_id, x, y) %>% \n  inner_join(env_variables_ps, by = \"cell_id\")\n\n# save as csv\nwrite_csv(env_variables_ps, \n          \"data/environmental-variables_prediction-surface.csv\", \n          na = \"\")\n\nKeeping these data in a data frame is a compact way to store them and will be required once we make model predictions in later chapters. However, we can always use the raster template to convert these environmental variables into a spatial format, for example, if we want to map them. Let’s look at how this works for percent landcover of class 14: deciduous broadleaf forest.\n\nforest_cover <- env_variables_ps %>% \n  # convert to spatial features\n  st_as_sf(coords = c(\"x\", \"y\"), crs = laea_crs) %>% \n  # rasterize points\n  rasterize(r, field = \"landcover_c14_pland\")\n\n# make a map\npar(mar = c(0.25, 0.25, 2, 0.25))\nplot(forest_cover, \n     axes = FALSE, box = FALSE, col = viridis(10), \n     main = paste(\"Percent Deciduous Broadleaf Forest\\n\",\n                  \"2021 MODIS Landcover\"))\n\n\n\n\n\n\n\n\n\n\n\n\nAmatulli, Giuseppe, Sami Domisch, Mao-Ning Tuanmu, Benoit Parmentier, Ajay Ranipeta, Jeremy Malczyk, and Walter Jetz. 2018. “A Suite of Global, Cross-Scale Topographic Variables for Environmental and Biodiversity Modeling.” Scientific Data 5 (March): 180040. https://doi.org/10.1038/sdata.2018.40.\n\n\nFriedl, Mark, and Damien Sulla-Menashe. 2015. “MCD12Q1 MODIS/Terra+Aqua Land Cover Type Yearly L3 Global 500m SIN Grid V006.” NASA EOSDIS Land Processes DAAC. https://doi.org/10.5067/MODIS/MCD12Q1.006."
  }
]