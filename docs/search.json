[
  {
    "objectID": "encounter.html#encouter-intro",
    "href": "encounter.html#encouter-intro",
    "title": "4  Encounter Rate",
    "section": "4.1 Introduction",
    "text": "4.1 Introduction\nIn this chapter we’ll estimate the encounter rate of Wood Thrush on eBird checklists in June in the state of Georgia. We define encounter rate as measuring the probability of an eBirder encountering a species on a standard eBird checklist.\nThe ecological metric we’re ultimately interested in is the probability that a species occurs at a site (i.e. the occupancy probability). This is usually not possible to estimate with semi-structured citizen science data like those from eBird because we typically can’t estimate absolute detectability. However, by accounting for much of the variation in detectability by including effort covariates in our model, the remaining unaccounted detectability will be more consistent across sites (Guillera-Arroita et al. 2015). Therefore, the encounter rate metric will be proportional to occupancy, albeit lower by some consistent amount. For some easily detectable species the difference between occurrence and actual occupancy rate will be small, and these encounter rates will approximate the actual occupancy rates of the species. For harder to detect species, the encounter rate may be substantially lower than the occupancy rate.\nRandom forests are a general purpose machine learning method applicable to a wide range of classification and regression problems, including the task at hand: classifying detection and non-detection of a species on eBird checklists. In addition to having good predictive performance, random forests are reasonably easy to use and have several efficient implementations in R. Prior to fitting a random forest model, we’ll demonstrate how to address issues of class imbalance and spatial bias using spatial subsampling on a regular grid. After fitting the model, we’ll assess its performance using a subset of data put aside for testing, and calibrate the model to ensure predictions are accurate. Finally, we’ll predict encounter rates throughout the study area and produce maps of these predictions."
  },
  {
    "objectID": "encounter.html#encounter-data",
    "href": "encounter.html#encounter-data",
    "title": "4  Encounter Rate",
    "section": "4.2 Data preparation",
    "text": "4.2 Data preparation\nLet’s get started by loading the necessary packages and data. If you worked through the previous chapters, you should have all the data required for this chapter. However, you may want to download the data package, and unzip it to your project directory, to ensure you’re working with exactly the same data as was used in the creation of this book.\n\nlibrary(ebirdst)\nlibrary(fields)\nlibrary(gridExtra)\nlibrary(mccf1)\nlibrary(ranger)\nlibrary(scam)\nlibrary(sf)\nlibrary(terra)\nlibrary(verification)\nlibrary(tidyverse)\n\n# set random number seed to insure fully repeatable results\nset.seed(1)\n\n# setup results directory for saved results\ndir.create(\"results/\", showWarnings = FALSE, recursive = )\n\n# habitat variables: landcover and elevation\nhabitat <- read_csv(\"data/environmental-variables_checklists.csv\")\n\n# zero-filled ebird data combined with habitat data\nchecklists <- read_csv(\"data/checklists-zf_woothr_june_us-ga.csv\") %>% \n  inner_join(habitat, by = \"checklist_id\")\n\n# prediction surface\npred_surface <- read_csv(\"data/environmental-variables_prediction-surface.csv\")\nr <- rast(\"data/prediction-surface.tif\") %>% \n  # this second rast() call removes all the values from the raster template\n  rast()\ncrs <- st_crs(r)\n\n# load gis data for making maps\nstudy_region <- read_sf(\"data/gis-data.gpkg\", \"ne_states\") %>% \n  filter(state_code == \"US-GA\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()\nne_land <- read_sf(\"data/gis-data.gpkg\", \"ne_land\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()\nne_country_lines <- read_sf(\"data/gis-data.gpkg\", \"ne_country_lines\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()\nne_state_lines <- read_sf(\"data/gis-data.gpkg\", \"ne_state_lines\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()"
  },
  {
    "objectID": "encounter.html#encounter-sss",
    "href": "encounter.html#encounter-sss",
    "title": "4  Encounter Rate",
    "section": "4.3 Spatiotemporal subsampling",
    "text": "4.3 Spatiotemporal subsampling\nAs discussed in the introduction, three of the challenges faced when using eBird data, are spatial bias, temporal bias, and class imbalance. Spatial and temporal bias refers to the tendency of eBird checklists to be distributed non-randomly in space and time, while class imbalance is the phenomenon that there are many more non-detections than detections for most species. All three can impact our ability to make reliable inferences from these data. Fortunately, all three can largely be addressed through subsampling the eBird data prior to modeling. In particular, we define an equal area, 3km by 3km square grid across the study region, then subsample detections and non-detections independently to ensure that we don’t lose too many detections. To address temporal bias, we’ll sample one detection and one non-detection checklist from each grid cell for each week of each year. Fortunately, the package ebirdst has a function grid_sample_stratified() that is specifically design to perform this type of sampling on eBird checklist data.\nBefore working with the real data, it’s instructive to look at a simple toy example, to see how this subsampling process works.\n\n# generate random points for a single week of the year\npts <- data.frame(longitude = runif(500, -0.1, 0.1),\n                  latitude = runif(500, -0.1, 0.1),\n                  day_of_year = sample(1:7, 500, replace = TRUE))\n\n# sample one checklist per grid cell\n# by default grid_sample() uses a 3km x 3km x 1 week grid\npts_ss <- grid_sample(pts)\n\n# generate polygons for the grid cells\nggplot(pts) +\n  aes(x = longitude, y = latitude) +\n  geom_point(size = 0.5) +\n  geom_point(data = pts_ss, col = \"red\") +\n  theme_bw()\n\n\n\n\n\n\n\n\nIn the above plot, the full set of points is shown in black and the subsampled points are shown in red. Now let’s apply exactly the same approach to subsampling the real eBird checklists; however, now we subsample temporally in addition to spatially, and sample detections and non-detections separately. Also, recall that we split the data 80/20 into train/test sets; using the sample_by argument we can independently sample from each set with grid_sample_stratified().\n\n# sample one checklist per 3km x 3km x 1 week grid for each year\n# sample detection/non-detection independently \nchecklists_ss <- grid_sample_stratified(checklists,\n                                        obs_column = \"species_observed\",\n                                        sample_by = \"type\")\n\nHow did this impact the prevalence of detections compared to non-detections?\n\n# original data\nnrow(checklists)\n#> [1] 43790\ncount(checklists, species_observed) %>% \n  mutate(percent = n / sum(n))\n#> # A tibble: 2 × 3\n#>   species_observed     n percent\n#>   <lgl>            <int>   <dbl>\n#> 1 FALSE            39392   0.900\n#> 2 TRUE              4398   0.100\n\n# after sampling\nnrow(checklists_ss)\n#> [1] 19170\ncount(checklists_ss, species_observed) %>% \n  mutate(percent = n / sum(n))\n#> # A tibble: 2 × 3\n#>   species_observed     n percent\n#>   <lgl>            <int>   <dbl>\n#> 1 FALSE            16622   0.867\n#> 2 TRUE              2548   0.133\n\nSo, the subsampling decreased the overall number of checklists by a factor of about four, but increased the prevalence of detections from 10.0% to 13.3%. This increase in detections will help the random forest model distinguish where birds are being observed; however, this does affect the prevalence rate of the detections in the data. As a result, the estimated probability of occurrence based on these subsampled data will be larger than the true occurrence rate. When examining the outputs from the models it will be important to recall that we altered the prevalence rate at this stage. Now let’s look at how the subsampling affects the spatial distribution of the observations.\n\n# convert checklists to spatial features\nall_pts <- checklists %>%  \n  st_as_sf(coords = c(\"longitude\",\"latitude\"), crs = 4326) %>%\n  st_transform(crs = crs) %>% \n  select(species_observed)\nss_pts <- checklists_ss %>%  \n  st_as_sf(coords = c(\"longitude\",\"latitude\"), crs = 4326) %>%\n  st_transform(crs = crs) %>% \n  select(species_observed)\nboth_pts <- list(before_ss = all_pts, after_ss = ss_pts)\n\n# map\np <- par(mfrow = c(1, 2))\nfor (i in seq_along(both_pts)) {\n  par(mar = c(0.25, 0.25, 0.25, 0.25))\n  # set up plot area\n  plot(st_geometry(both_pts[[i]]), col = NA)\n  # contextual gis data\n  plot(ne_land, col = \"#dddddd\", border = \"#888888\", lwd = 0.5, add = TRUE)\n  plot(study_region, col = \"#cccccc\", border = NA, add = TRUE)\n  plot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\n  plot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\n  # ebird observations\n  # not observed\n  plot(st_geometry(both_pts[[i]]),\n       pch = 19, cex = 0.1, col = alpha(\"#555555\", 0.25),\n       add = TRUE)\n  # observed\n  plot(filter(both_pts[[i]], species_observed) %>% st_geometry(),\n       pch = 19, cex = 0.3, col = alpha(\"#4daf4a\", 0.5),\n       add = TRUE)\n  # legend\n  legend(\"bottomright\", bty = \"n\",\n         col = c(\"#555555\", \"#4daf4a\"),\n         legend = c(\"Non-detection\", \"Detection\"),\n         pch = 19)\n  box()\n  par(new = TRUE, mar = c(0, 0, 3, 0))\n  if (names(both_pts)[i] == \"before_ss\") {\n    title(\"Wood Thrush eBird Observations\\nBefore subsampling\")\n  } else {\n    title(\"After subsampling\")\n  }\n}\npar(p)\n\n\n\n\n\n\n\n\nFor Wood Thrush, subsampling the detections and non-detections independently is sufficient for dealing with class imbalance. You can assess the impact of class imbalance by looking at the prevalence rates and examining whether the models are good at predicting to validation data. For species that are extremely rare, it may be worthwhile considering keeping all detections or even oversampling detections (Robinson, Ruiz-Gutierrez, and Fink 2018). In doing this, be aware that some of your species detections will not be independent, which could lead to overfitting of the data. Overall, when thinking about the number of detections and the prevalence rate, it’s important to consider both the ecology and detectability of the focal species, and the behavior of observers towards this species."
  },
  {
    "objectID": "encounter.html#encounter-rf",
    "href": "encounter.html#encounter-rf",
    "title": "4  Encounter Rate",
    "section": "4.4 Random forests",
    "text": "4.4 Random forests\nNow we’ll use a random forests model to relate detection/non-detection of Wood Thrush to the habitat covariates (MODIS land cover and elevation), while also accounting for variation in detectability by including a suite of effort covariates. At this stage, we filter to just the training set, leaving the test set to assess predictive performance later.\n\nchecklists_train <- checklists_ss %>% \n  filter(type == \"train\") %>% \n  # select only the columns to be used in the model\n  select(species_observed,\n         year, day_of_year, hours_of_day,\n         effort_hours, effort_distance_km, effort_speed_kmph,\n         number_observers, \n         starts_with(\"pland_\"),\n         starts_with(\"ed_\"),\n         starts_with(\"elevation_\"))\n\nAlthough we were able to partially address the issue of class imbalance via subsampling, detections still only make up 13.3% of observations, and for rare species this number will be even lower. Most classification algorithms aim to minimize the overall error rate, which results in poor predictive performance for rare classes (Chen, Liaw, and Breiman 2004). To address this issue, we’ll use a balanced random forest approach, a modification of the traditional random forest algorithm designed to handle imbalanced data. In this approach, each of the trees that makes up the random forest is generated using a random sample of the data chosen such that there is an equal number of the detections (the rare class) and non-detections (the common class). To use this approach, we’ll need to calculate the proportion of detections in the dataset.\n\ndetection_freq <- mean(checklists_train$species_observed)\n\nThere are several packages for fitting random forests in R; however, we’ll use ranger, which is a blazingly fast implementation with all the features we need. To fit a balanced random forest, we use the sample.fraction parameter to instruct ranger to grow each tree based on a random sample of the data that has an equal number of detections and non-detections. Specifying this is somewhat obtuse, because we need to tell ranger the proportion of the total data set to sample for non-detections and detections, and when this proportion is the same as the proportion of the rarer class–the detections–then then ranger will sample from all of the rarer class but from an equally sized subset of the more common non-detections. We use replace = TRUE to ensure that it’s a bootstrap sample. We’ll also ask ranger to predict probabilities, rather than simply returning the most probable class, with probability = TRUE.\n\n# ranger requires a factor response to do classification\ner_model <- ranger(formula =  as.factor(species_observed) ~ ., \n                   data = checklists_train,\n                   importance = \"impurity\",\n                   probability = TRUE,\n                   replace = TRUE, \n                   sample.fraction = c(detection_freq, detection_freq))\n\n\n4.4.1 Calibration\nFor various reasons, the predicted probabilities from models do not always align with the observed frequencies of detections. For example, we would hope that if we look at all sites with a estimated probability of encounter of 0.2, that 20% of these would record the species. However, these probabilities are not always so well aligned. This will clearly be the case in our example, because we have deliberately inflated the prevalence of detection records in the data through the spatiotemporal subsampling process. We can produce a calibration of the predictions, which can be a useful diagnostic tool to understand the model predictions, and in some cases can be used to realign the predictions with observations. For information on calibration in species distribution models see Vaughan and Ormerod (2005) and for more fundamental references on calibration see Platt (1999), Murphy (1973), and Niculescu-Mizil and Caruana (2005).\nTo view the calibration of our model results, we predict encounter rate for each checklist in the training set, then fit a binomial Generalized Additive Model (GAM) with the real observed encounter rate as the response and the predicted encounter rate as the predictor variable. Whereas GLMs fit a linear relationship between a response and predictors, GAMs allow non-linear relationships. Although GAMs provide a degree of flexibility, in some situations they may overfit and provide unrealistic and unhelpful calibrations. We have a strong a priori expectation that higher real values will also be associated with higher estimated encounter rates. In order to maintain the ranking of predictions, it is important that we respect this ordering and to do this we’ll use a GAM that is constrained to only increase. To fit the GAM, we’ll use the R package scam, so the shape can be constrained to be monotonically increasing. Note that predictions from ranger are in the form of a matrix of probabilities for each class, and we want the probability of detections, which is the second column of this matrix.\n\n# predicted encounter rate predictions based on out of bag samples\ner_pred <- er_model$predictions[, 2]\n# observed detection, converted back from factor\ndet_obs <- as.integer(checklists_train$species_observed)\n# construct a data frame to train the scam model\nobs_pred <- tibble(obs = det_obs, pred = er_pred)\n\n# fit calibration model\ncalibration_model <- scam(obs ~ s(pred, k = 6, bs = \"mpi\"), \n                          gamma = 2,\n                          data = obs_pred)\n\nTo use the calibration model as a diagnostic tool, we’ll group the predicted encounter rates into bins, then calculate the mean predicted and observed encounter rates within each bin. This can be compared to predictions from the calibration model.\n\n# group the predicted encounter rate into bins of width 0.02\n# then calculate the mean observed encounter rates in each bin\ner_breaks <- seq(0, 1, by = 0.02)\nmean_er <- obs_pred %>%\n  mutate(er_bin = cut(pred, breaks = er_breaks, include.lowest = TRUE)) %>%\n  group_by(er_bin) %>%\n  summarise(n_checklists = n(),\n            pred = mean(pred), \n            obs = mean(obs),\n            .groups = \"drop\")\n\n# make predictions from the calibration model\ncal_pred <- data.frame(pred = er_breaks)\ncal_pred <- predict(calibration_model, cal_pred, type = \"response\") %>% \n  bind_cols(cal_pred, calibrated = .)\n\n# compared binned mean encouter rates to calibration model\nggplot(cal_pred) +\n  aes(x = pred, y = calibrated) +\n  geom_abline(slope = 1, intercept = 0, linetype = \"dashed\") +\n  geom_line(color = \"blue\") +\n  geom_point(data = mean_er, \n             aes(x = pred, y = obs),\n             size = 2, alpha = 0.6,\n             show.legend = FALSE) +\n  labs(x = \"Estimated encounter rate\",\n       y = \"Observed encounter rate\",\n       title = \"Calibration model\") +\n  coord_equal(xlim = c(0, 1), ylim = c(0, 1))\n\n\n\n\n\n\n\n\nFrom this plot we can clearly see that the estimated encounter rates are mostly much larger than the observed encounter rates (all points fall below the dashed \\(x = y\\) line. So we see that the model is not well calibrated. However, we do see from the points that the relative ranking of predictions is largely good: sites with estimated higher encounter rate do mostly have higher observed encounter rates.\nFrom this we have learnt that the model is good at distinguishing sites with high rates from those with low rates. For those readers familiar with using AUC scores to assess the quality of species distribution models, the graph is telling us that the model should have a high AUC value. However, the model is not so good at estimating encounter rates accurately.\nIf accurate encounter rates are required, and the calibration model is strong (close fit of points to the line in the figure above), then the calibration model can be used to calibrate the estimates from the random forest model, so they are adjusted to match the observed encounter rates more closely. The calibrated random forest model is the combination of the original random forest model followed by the calibration model.\nIf you’re using this model to calibrate your estimates, notice that the calibration curve can produce probabilities greater than 1 and less than 0, so when applying the calibration we also need to restrict the predictions to be between 0 and 1. It’s possible to run a logistic regression for the calibration to remove these predictions less than 0 or greater than 1; however, we’ve found the Gaussian constrained GAM to be more stable than the logistic constrained GAM.\n\n\n4.4.2 Thresholding\nThe random forest model produces continuous estimates of encounter rate from 0-1. However, for many applications, including assessing model performance, we’ll need to reclassify this continuous probability to a binary presence/absence estimate. This reclassification is done by setting a threshold above which the species is predicted to be absent. The threshold is typically chosen to maximize a performance metric such as the Kappa statistic or the area under the ROC curve. However, for class imbalanced data, such as eBird data where non-detections are much more common, many of these metrics can inflate performance by over-weighting the more common class (Cao, Chicco, and Hoffman 2020). To mitigate these issues, we suggest a threshold setting method using the MCC-F1 curve. This method plots Matthews correlation coefficient (MCC) against the F1 score for a range of possible threshold, then chooses the threshold where the curve is closest to the point of perfect performance. The R packge mccf1 implements the method.\n\n# mcc and fscore calculation for various thresholds\nmcc_f1 <- mccf1(\n  # observed detection/non-detection\n  response = obs_pred$obs,\n  # predicted encounter rate from random forest\n  predictor = obs_pred$pred)\n\n# identify best threshold\nmcc_f1_summary <- summary(mcc_f1)\n#>  mccf1_metric best_threshold\n#>          0.39           0.58\nthreshold <- mcc_f1_summary$best_threshold[1]\n\n\n\n4.4.3 Assessment\nTo assess the quality of the calibrated random forest model, we’ll validate the model’s ability to predict the observed patterns of detection using independent validation data (i.e. the 20% test data set). We’ll use a range of predictive performance metrics (PPMs) to compare the predictions to the actual observations. Mean squared error (MSE) and Spearman’s rank correlation coefficient apply to the calibrated encounter rate estimates. The remaining metrics apply to the binary detection/non-detection predictions: sensitivity, specificity, Precision-Recall AUC, Kappa, F1 score, and MCC.\n\n# get the test set held out from training\nchecklists_test <- filter(checklists_ss, type == \"test\") %>% \n  mutate(species_observed = as.integer(species_observed))\n\n# predict on test data using random forest model\npred_er <- predict(er_model, data = checklists_test, type = \"response\")\n# extract probability of detection\npred_er <- pred_er$predictions[, 2]\n# convert to binary using the threshold\npred_binary <- as.integer(pred_er > threshold)\n# calibrate\npred_calibrated <- predict(calibration_model, \n                           newdata = data.frame(pred = pred_er), \n                           type = \"response\") %>% \n  as.numeric()\nobs_pred_test <- data.frame(id = seq_along(pred_calibrated),\n                            # actual detection/non-detection\n                            obs = as.integer(checklists_test$species_observed),\n                            # binary detection/on-detection prediction\n                            pred_binary = pred_binary,\n                            # calibrated encounter rate\n                            pred_calibrated = pred_calibrated) %>%\n  # constrain probabilities to 0-1\n  mutate(pred_calibrated = pmin(pmax(pred_calibrated, 0), 1))\n\n# mean squared error (mse)\nmse <- mean((obs_pred_test$obs - obs_pred_test$pred_calibrated)^2, na.rm = TRUE)\n\n# spearman correlation\nspearman <- cor(obs_pred_test$pred_calibrated, obs_pred_test$obs, \n                method = \"spearman\")\n\n# precision-recall auc\nem <- precrec::evalmod(scores = obs_pred_test$pred_binary, \n                       labels = obs_pred_test$obs)\npr_auc <- precrec::auc(em) %>% \n  filter(curvetypes == \"PRC\") %>% \n  pull(aucs)\n\n# calculate metrics for binary prediction: kappa, sensitivity, specificity\npa_metrics <- obs_pred_test %>% \n  select(id, obs, pred_binary) %>% \n  PresenceAbsence::presence.absence.accuracy(na.rm = TRUE, st.dev = FALSE)\n\n# mcc and f1\nmcc_f1 <- calculate_mcc_f1(obs_pred_test$obs, obs_pred_test$pred_binary)\n\n# combine metrics together\nppms <- tibble(\n  mse = mse,\n  spearman = spearman,\n  sensitivity = pa_metrics$sensitivity,\n  specificity = pa_metrics$specificity,\n  kappa = pa_metrics$Kappa,\n  pr_auc = pr_auc,\n  mcc = mcc_f1$mcc,\n  f1 = mcc_f1$f1\n)\nknitr::kable(pivot_longer(ppms, everything()), digits = 3)\n\n\n\n\nname\nvalue\n\n\n\n\nmse\n0.080\n\n\nspearman\n0.387\n\n\nsensitivity\n0.616\n\n\nspecificity\n0.870\n\n\nkappa\n0.383\n\n\npr_auc\n0.302\n\n\nmcc\n0.398\n\n\nf1\n0.470\n\n\n\n\n\nEach of these metrics can inform us about different aspects of the model fit. The objectives of your study will determine which of these metrics is most important. For example, if you want to ensure that the model definitely includes all areas where the species occurs, you would seek to have high sensitivity. Alternatively, if you want to ensure that the species does indeed occur in all places the model predicts the species to occurr (for example, when identifying areas for conservation action), you would seek to maximise specificity."
  },
  {
    "objectID": "encounter.html#encounter-habitat",
    "href": "encounter.html#encounter-habitat",
    "title": "4  Encounter Rate",
    "section": "4.5 Habitat associations",
    "text": "4.5 Habitat associations\nFrom the random forest model, we can glean two important sources of information about the association between Wood Thrush detection and features of their local environment. First, predictor importance is a measure of the predictive power of each variable used as a predictor in the model, and is calculated as a byproduct of fitting a random forest model. Second, partial dependence plots estimate the marginal effect of one predictor holding all other predictors constant.\n\n4.5.1 Predictor importance\nDuring the process of fitting a random forest model, some variables are removed at each node of the trees that make up the random forest. Predictor importance is based on the mean decrease in accuracy of the model when a given predictor is not used. It’s technically an average Gini index, but essentially larger values indicate that a predictor is more important to the model.\n\npi <- enframe(er_model$variable.importance, \"predictor\", \"importance\") %>% \n  # only show the top 20\n  slice_max(order_by = importance, n = 20)\n# plot\nggplot(pi) + \n  aes(x = fct_reorder(predictor, importance), y = importance) +\n  geom_col() +\n  geom_hline(yintercept = 0, linewidth = 2, colour = \"#555555\") +\n  scale_y_continuous(expand = c(0, 0)) +\n  coord_flip() +\n  labs(x = NULL, \n       y = \"Predictor Importance (Gini Index)\") +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        panel.grid.major.x = element_line(colour = \"#cccccc\", linewidth = 0.5))\n\n\n\n\n\n\n\n\nThe most important predictors of detection/non-detection are often effort variables. Indeed, that’s the case here: checklist duration, distance traveled, and start time (hours_of_day) all appear in the top 5 predictors. This is not suprising: going out at the right time of day and expending more effort searching will lead to a higher probabilty of detecting Wood Thrush. Focusing on the habitat variables, both elevation variables have high importance, and the top habitat variables are from deciduous broadleaf forest and woody savanna. Note however, that high importance doesn’t tell us the direction of the relationship with detection, for that we’ll have to look at partial dependence plots.\nLet’s grab the top 9 most important predictors, which we’ll need in the next section.\n\n# top 9 predictors other than date\ntop_pred <- pi %>% \n  filter(!predictor %in% c(\"year\", \"day_of_year\")) %>% \n  slice_max(order_by = importance, n = 9) %>% \n  arrange(desc(importance))\n\n\n\n4.5.2 Partial dependence\nPartial dependence plots show the marginal effect of a given predictor on encounter rate averaged across the other predictors. These plots are generated by predicting encounter rate at a regular sequence of points across the full range of values of a given predictor. At each predictor value, predictions of encounter rate are made for a random subsample of the training dataset with the focal predictor fixed, but all other predictors left as is. The encounter rate predictions are then averaged across all the checklists in the training dataset giving an estimate of the average encounter rate at a specific value of the focal predictor. This is a cumbersome process, but we provide a function below that does all the hard work for you! This function takes the following arguments:\n\npredictor: the name of the predictor to calculate partial dependence for\nmodel: the encounter rate model\ndata: the original data used to train the model\nx_res: the resolution of the grid over which to calculate the partial dependence, i.e. the number of points between the minimum and maximum values of the predictor to evaluate partial dependence at\nn: number of points to subsample from the training data\n\n\n# function to calculate partial dependence for a single predictor\ncalculate_pd <- function(predictor, model, data, \n                         x_res = 25, n = 1000) {\n  # create prediction grid using quantiles\n  x_grid <- quantile(data[[predictor]],\n                     probs = seq(from = 0, to = 1, length = x_res),\n                     na.rm = TRUE)\n  # remove duplicates\n  x_grid <- x_grid[!duplicated(signif(x_grid, 8))]\n  x_grid <- unname(unique(x_grid))\n  grid <- data.frame(predictor = predictor, x = x_grid)\n  names(grid) <- c(\"predictor\", predictor)\n  \n  # subsample training data\n  n <- min(n, nrow(data))\n  data <- data[sample(seq.int(nrow(data)), size = n, replace = FALSE), ]\n  \n  # drop focal predictor from data\n  data <- data[names(data) != predictor]\n  grid <- merge(grid, data, all = TRUE)\n  \n  # predict\n  p <- predict(model, data = grid)\n  \n  # summarize\n  pd <- grid[, c(\"predictor\", predictor)]\n  names(pd) <- c(\"predictor\", \"x\")\n  pd$encounter_rate <- p$predictions[, 2]\n  pd <- dplyr::group_by(pd, predictor, x) %>% \n    dplyr::summarise(encounter_rate = mean(encounter_rate, na.rm = TRUE),\n                     .groups = \"drop\")\n  \n  return(pd)\n}\n\nNow we’ll use this function to calculate partial dependence for the top 9 predictors.\n\n# calculate partial dependence for each predictor\n# map is used to iteratively apply calculate_pd to each predictor\npd <- NULL\nfor (predictor in top_pred$predictor) {\n  pd <- calculate_pd(predictor, model = er_model, data = checklists_train) %>% \n    bind_rows(pd, .)\n}\n\n# calibrate predictions\npd$encounter_rate <- predict(calibration_model, \n                             newdata = tibble(pred = pd$encounter_rate), \n                             type = \"response\") %>% \n  as.numeric()\n\n# plot\nggplot(pd) +\n  aes(x = x, y = encounter_rate) +\n  geom_line() +\n  geom_point() +\n  scale_y_continuous(labels = scales::percent) +\n  facet_wrap(~ as_factor(predictor), nrow = 3, scales = \"free\") +\n  labs(x = NULL, y = \"Encounter Rate\") +\n  theme_minimal() +\n  theme_minimal() +\n  theme(panel.grid = element_blank(),\n        axis.line = element_line(color = \"grey60\"),\n        axis.ticks  = element_line(color = \"grey60\"))\n\n\n\n\n\n\n\n\nThere are a range of interesting responses here. As seen in Section @ref(ebird-explore), the encounter rate for Wood Thrush peaks early in the morning when they’re most likely to be singing, then quickly drops off in the middle of the day, before slightly increasing in the evening. Some other predictors show a more smoothly increasing relationship with encounter rate, for example, as the landscape contains more deciduous forest, the encounter rate increases.\nThe random forest model has a number of interactions, which are not displayed in these partial dependence plots. When interpreting these, bear in mind that there are likely some more complex interaction effects beneath these individual plots."
  },
  {
    "objectID": "encounter.html#encounter-predict",
    "href": "encounter.html#encounter-predict",
    "title": "4  Encounter Rate",
    "section": "4.6 Prediction",
    "text": "4.6 Prediction\nNow for the fun part: let’s use the calibrated random forest model to make a map of Wood Thrush encounter rate in Georgia! In Section @ref(envvar-pred), we created a prediction surface consisting of the habitat variables summarized on a regular grid of points across the study region. In this section, we’ll make predictions of encounter rate at these points. However, first we need to bring effort variables into this prediction surface. We’ll make predictions for a standard eBird checklist: a 1 km, 2 hour traveling count at the peak time of day for detecting this species. Finally, we’ll make these predictions for June 15, 2022, the middle of our June focal window for the latest year for which we have eBird data.\nTo find the time of day with the highest detection probability, we can look for the peak of the partial dependence plot. The one caveat to this approach is that it’s important we focus on times of day for which there are enough data to make predictions. In particular, there’s an increasing trend in detectability with earlier start times, and few checklists late at night, which can cause the model to incorrectly extrapolate that trend to show highest detectability at night. Let’s start by looking at a plot to see if this is happening here.\n\n# find peak time of day from partial dependence\npd_time <- calculate_pd(\"hours_of_day\",\n                        model = er_model, \n                        data = checklists_train,\n                        # make estimates at 30 minute intervals\n                        # using a subset of the training dataset\n                        x_res = 2 * 24, n = 1000) %>% \n  select(hours_of_day = x, encounter_rate)\n\n# histogram\ng_hist <- ggplot(checklists_train) +\n  aes(x = hours_of_day) +\n  geom_histogram(binwidth = 1, center = 0.5, color = \"grey30\",\n                 fill = \"grey50\") +\n  scale_x_continuous(breaks = seq(0, 24, by = 3)) +\n  scale_y_continuous(labels = scales::comma) +\n  labs(x = \"Hours since midnight\",\n       y = \"# checklists\",\n       title = \"Distribution of observation start times\")\n\n# gam\ng_pd <- ggplot(pd_time) +\n  aes(x = hours_of_day, y = encounter_rate) +\n  geom_line() +\n  scale_x_continuous(breaks = seq(0, 24, by = 3)) +\n  labs(x = \"Hours since midnight\",\n       y = \"Probability of reporting\",\n       title = \"Observation start time partial dependence\")\n\n# combine\ngrid.arrange(g_hist, g_pd)\n\n\n\n\n\n\n\n\nThe peak probability of reporting is very close to the time of day during which the abundance of reports starts to increase, but from these graphs it is not entirely clear that the early morning peak in reports is well substantiated by abundant data. Let’s instead look for the peak time within hours of the day that contain at least 1% of the training data.\n\n# hours with at least 1% of checklists\nsearch_times <- checklists_train %>% \n  mutate(hours_of_day = floor(hours_of_day)) %>%\n  count(hours_of_day) %>% \n  mutate(pct = n / sum(n)) %>% \n  filter(pct >= 0.01)\n\n# constrained peak time\nt_peak <- pd_time %>% \n  filter(floor(hours_of_day) %in% search_times$hours_of_day) %>% \n  slice_max(order_by = encounter_rate) %>% \n  pull(hours_of_day)\nt_peak\n#> [1] 6.17\n\n\n\n\nBased on this analysis, the best time for detecting Wood Thrush is at 6:10 AM. Now we use this time to make predictions. This is equivalent to many eBirders all conducting a checklist within different grid cells on June 15 at 6:10 AM. We also add the other effort variables to the prediction dataset.\n\n# add effort covariates to prediction \npred_surface_eff <- pred_surface %>% \n  mutate(observation_date = ymd(\"2022-06-15\"),\n         year = year(observation_date),\n         day_of_year = yday(observation_date),\n         hours_of_day = t_peak,\n         effort_hours = 2,\n         effort_distance_km = 1,\n         effort_speed_kmph = 0.5,\n         number_observers = 1)\n\n# predict\npred_er <- predict(er_model, data = pred_surface_eff, type = \"response\")\npred_er <- pred_er$predictions[, 2]\n# apply calibration\npred_er_cal <- predict(calibration_model, \n                       data.frame(pred = pred_er), \n                       type = \"response\") %>% \n  as.numeric()\n# add encounter rate estimate to prediction surface\npredictions <- bind_cols(pred_surface_eff, \n                         encounter_rate = pred_er_cal) %>% \n  select(cell_id, x, y, encounter_rate) %>% \n  mutate(encounter_rate = pmin(pmax(encounter_rate, 0), 1))\n\nNext, we’ll convert this data frame to spatial features using sf, then rasterize the points using the prediction surface raster template.\n\nr_pred <- predictions %>% \n  # convert to spatial features\n  st_as_sf(coords = c(\"x\", \"y\"), crs = crs) %>% \n  select(encounter_rate) %>% \n  # rasterize\n  rasterize(r, field = \"encounter_rate\", fun = \"mean\") %>% \n  setNames(\"encounter_rate\")\n\n# save the raster\nr_pred <- writeRaster(r_pred, \"results/encounter-rate_woothr.tif\", \n                      overwrite = TRUE,\n                      gdal = \"COMPRESS=DEFLATE\")\n\nFinally, we can make a map of these encounter rate predictions!\n\npar(mar = c(3.5, 0.25, 0.25, 0.25))\n# set up plot area\nplot(study_region, col = NA, border = NA)\nplot(ne_land, col = \"#dddddd\", border = \"#888888\", lwd = 0.5, add = TRUE)\n\n# define quantile breaks\nbrks <- global(r_pred, fun = quantile, \n               probs = seq(0, 1, 0.1), na.rm = TRUE) %>% \n  as.numeric() %>% \n  unique()\n# label the bottom, middle, and top value\nlbls <- round(c(0, median(brks), max(brks)), 2)\n# ebird status and trends color palette\npal <- abundance_palette(length(brks) - 1)\nplot(r_pred, \n     col = pal, breaks = brks, \n     maxpixels = ncell(r_pred),\n     legend = FALSE, axes = FALSE, bty = \"n\",\n     add = TRUE)\n\n# borders\nplot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\nplot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\nplot(study_region, border = \"#000000\", col = NA, lwd = 1, add = TRUE)\nbox()\n\n# legend\npar(new = TRUE, mar = c(0, 0, 0, 0))\ntitle <- \"Wood Thrush Encounter Rate (June 2022)\"\nimage.plot(zlim = c(0, 1), legend.only = TRUE, \n           col = pal, breaks = seq(0, 1, length.out = length(brks)),\n           smallplot = c(0.25, 0.75, 0.06, 0.09),\n           horizontal = TRUE,\n           axis.args = list(at = c(0, 0.5, 1), labels = lbls,\n                            fg = \"black\", col.axis = \"black\",\n                            cex.axis = 0.75, lwd.ticks = 0.5,\n                            padj = -1.5),\n           legend.args = list(text = title,\n                              side = 3, col = \"black\",\n                              cex = 1, line = 0))\n\n\n\n\n\n\n\n\n\n\n\n\nCao, Chang, Davide Chicco, and Michael M. Hoffman. 2020. “The MCC-F1 Curve: A Performance Evaluation Technique for Binary Classification.” https://doi.org/10.48550/ARXIV.2006.11278.\n\n\nChen, Chao, Andy Liaw, and Leo Breiman. 2004. “Using Random Forest to Learn Imbalanced Data.” University of California, Berkeley 110 (1-12): 24.\n\n\nGuillera-Arroita, Gurutzeta, José J. Lahoz-Monfort, Jane Elith, Ascelin Gordon, Heini Kujala, Pia E. Lentini, Michael A. McCarthy, Reid Tingley, and Brendan A. Wintle. 2015. “Is My Species Distribution Model Fit for Purpose? Matching Data and Models to Applications.” Global Ecology and Biogeography 24 (3): 276–92.\n\n\nMurphy, Allan H. 1973. “A New Vector Partition of the Probability Score.” Journal of Applied Meteorology 12 (4): 595–600. https://doi.org/10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2.\n\n\nNiculescu-Mizil, Alexandru, and Rich Caruana. 2005. “Predicting Good Probabilities with Supervised Learning.” In Proceedings of the 22nd International Conference on Machine Learning, 625–32. ACM.\n\n\nPlatt, John. 1999. “Probabilistic Outputs for Support Vector Machines and Comparisons to Regularized Likelihood Methods.” Advances in Large Margin Classifiers 10 (3): 61–74.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, and Daniel Fink. 2018. “Correcting for Bias in Distribution Modelling for Rare Species Using Citizen Science Data.” Diversity and Distributions 24 (4): 460–72. https://doi.org/10.1111/ddi.12698.\n\n\nVaughan, I. P., and S. J. Ormerod. 2005. “The Continuing Challenges of Testing Species Distribution Models.” Journal of Applied Ecology 42 (4): 720–30. https://doi.org/10.1111/j.1365-2664.2005.01052.x."
  },
  {
    "objectID": "abundance.html#encounter-data",
    "href": "abundance.html#encounter-data",
    "title": "5  Relative Abundance",
    "section": "5.1 Data preparation",
    "text": "5.1 Data preparation\nLet’s get started by loading the necessary packages and data. If you worked through the previous chapters, you should have all the data required for this chapter. However, you may want to download the data package, and unzip it to your project directory, to ensure you’re working with exactly the same data as was used in the creation of this book.\n\nlibrary(ebirdst)\nlibrary(fields)\nlibrary(gridExtra)\nlibrary(mccf1)\nlibrary(ranger)\nlibrary(scam)\nlibrary(sf)\nlibrary(terra)\nlibrary(tidyverse)\n\n# set random number seed to insure fully repeatable results\nset.seed(1)\n\n# setup results directory for saved results\ndir.create(\"results/\", showWarnings = FALSE, recursive = TRUE)\n\n# habitat variables: landcover and elevation\nhabitat <- read_csv(\"data/environmental-variables_checklists.csv\")\n\n# zero-filled ebird data combined with habitat data\nchecklists <- read_csv(\"data/checklists-zf_woothr_june_us-ga.csv\") %>% \n  inner_join(habitat, by = \"checklist_id\")\n\n# prediction surface\npred_surface <- read_csv(\"data/environmental-variables_prediction-surface.csv\")\nr <- rast(\"data/prediction-surface.tif\") %>% \n  # this second rast() call removes all the values from the raster template\n  rast()\ncrs <- st_crs(r)\n\n# load gis data for making maps\nstudy_region <- read_sf(\"data/gis-data.gpkg\", \"ne_states\") %>% \n  filter(state_code == \"US-GA\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()\nne_land <- read_sf(\"data/gis-data.gpkg\", \"ne_land\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()\nne_country_lines <- read_sf(\"data/gis-data.gpkg\", \"ne_country_lines\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()\nne_state_lines <- read_sf(\"data/gis-data.gpkg\", \"ne_state_lines\") %>% \n  st_transform(crs = crs) %>% \n  st_geometry()\n\nNext we’ll perform a round of spatiotemporal subsampling on the data to reduce bias.\n\n# sample one checklist per 3km x 3km x 1 week grid for each year\n# sample detection/non-detection independently \nchecklists_ss <- grid_sample_stratified(checklists,\n                                        obs_column = \"species_observed\",\n                                        sample_by = \"type\")\n\nFinally, we’ll remove the 20% of checklists held aside for testing and select only the columns we intend to use as predictors in the model training.\n\nchecklists_train <- checklists_ss %>% \n  filter(type == \"train\") %>% \n  # select only the columns to be used in the model\n  select(species_observed, observation_count,\n         year, day_of_year, hours_of_day,\n         effort_hours, effort_distance_km, effort_speed_kmph,\n         number_observers, \n         starts_with(\"pland_\"),\n         starts_with(\"ed_\"),\n         starts_with(\"elevation_\"))"
  },
  {
    "objectID": "abundance.html#abundance-hurdle",
    "href": "abundance.html#abundance-hurdle",
    "title": "5  Relative Abundance",
    "section": "5.2 Hurdle model",
    "text": "5.2 Hurdle model\nFor this two-step hurdle model, we’ll start by training exactly the same encounter rate model as in the previous chapter. Then we’ll subset the eBird checklist to only those where the species was detected or predicted to occur by the encounter rate model. We’ll use this subset of the data to train a second random forest model for expected count. Finally we’ll combine the results of the two steps together to produce estimates of relative abundance.\n\n5.2.1 Step 1: Encounter rate\nIf you haven’t done so, read Chapter @ref(encounter) for details on the calibrated encounter rate model. Here we repeat the process of modeling encounter rate in a compressed form.\n\n# calculate detection frequency for the balance random forest\ndetection_freq <- mean(checklists_train$species_observed)\n\n# train a random forest model for encounter rate\ntrain_er <- select(checklists_train, -observation_count)\ner_model <- ranger(formula =  as.factor(species_observed) ~ ., \n                   data = train_er,\n                   importance = \"impurity\",\n                   probability = TRUE,\n                   replace = TRUE,\n                   sample.fraction = c(detection_freq, detection_freq))\n\n# select the mcc-f1 optimizing occurrence threshold\nobs_pred <- tibble(obs = as.integer(train_er$species_observed), \n                   pred = er_model$predictions[, 2])\nmcc_f1 <- mccf1(response = obs_pred$obs, predictor = obs_pred$pred)\nmcc_f1_summary <- summary(mcc_f1)\nthreshold <- mcc_f1_summary$best_threshold[1]\n\n# calibration model\ncalibration_model <- scam(obs ~ s(pred, k = 6, bs = \"mpi\"), \n                          gamma = 2,\n                          data = obs_pred)\n#>  mccf1_metric best_threshold\n#>         0.396          0.572\n\n\n\n5.2.2 Step 2: Expected count\nFor the second step, we train a random forests model to estimate the expected count of individuals on eBird checklists where the species was detected or predicted to be detected by the encounter rate model. So, we’ll start by subsetting the data to just these checklists. In addition, we’ll remove any observations for which the observer reported that Wood Thrush was present, but didn’t report a count of the number of individuals (coded as a count of “X” in the eBird database, but converted to NA in our dataset).\n\n# attach the predicted encounter rate based on out of bag samples\ntrain_er <- checklists_train\ntrain_er$pred_er <- er_model$predictions[, 2]\n# subset to only observed or predicted detections\ntrain_count <- train_er %>% \n  filter(!is.na(observation_count),\n         observation_count > 0 | pred_er > threshold) %>% \n  select(-species_observed, -pred_er)\n\nWe’ve found that including predicted encounter rate as a predictor in the count model improves predictive performance. So, with this in mind, we predict encounter rate for the training dataset and add it as an additional column.\n\npredicted_er <- predict(er_model, data = train_count, type = \"response\")\npredicted_er <- predicted_er$predictions[, 2]\ntrain_count$predicted_er <- predicted_er\n\nFinally, we train a random forests model to estimate count. This is superficially very similar to the random forests model for encounter rate; however, for count we’re using a regression random forest while for encounter rate we used a balanced classification random forest.\n\ncount_model <- ranger(formula = observation_count ~ .,\n                      data = train_count,\n                      importance = \"impurity\",\n                      replace = TRUE)\n\n\n\n5.2.3 Assessment\nIn the previous chapter we calculate a suite of predictive performance metrics for the encounter rate model. These metrics should also be considered when modeling relative; however, here we won’t duplicate these metrics here. Instead we’ll calculate Spearman’s rank correlation coefficient for both count and relative abundance and Pearson correlation coefficient for the log of count and relative abundance. We’ll start by estimating encounter rate, count, and relative abundance for the test dataset.\n\n# get the test set held out from training\nchecklists_test <- filter(checklists_ss, type == \"test\") %>% \n  mutate(species_observed = as.integer(species_observed)) %>% \n  filter(!is.na(observation_count))\n\n# estimate encounter rate for test data\npred_er <- predict(er_model, data = checklists_test, type = \"response\")\n# extract probability of detection\npred_er <- pred_er$predictions[, 2]\n# convert to binary using the threshold\npred_binary <- as.integer(pred_er > threshold)\n# calibrate\npred_calibrated <- predict(calibration_model, \n                           newdata = data.frame(pred = pred_er), \n                           type = \"response\") %>% \n  as.numeric()\n# constrain probabilities to 0-1\npred_calibrated <- pmin(pmax(pred_calibrated, 0), 1)\n\n# add predicted encounter rate required for count estimates\nchecklists_test$predicted_er <- pred_er\n# estimate count\npred_count <- predict(count_model, data = checklists_test, type = \"response\")\npred_count <- pred_count$predictions\n\n# relative abundance is the product of encounter rate and count\npred_abundance <- pred_calibrated * pred_count\n\n# combine all estimates together\nobs_pred_test <- data.frame(\n  id = seq_along(pred_abundance),\n  # actual detection/non-detection\n  obs_detection = as.integer(checklists_test$species_observed),\n  obs_count = checklists_test$observation_count,\n  # model estimates\n  pred_binary = pred_binary,\n  pred_er = pred_calibrated,\n  pred_count = pred_count,\n  pred_abundance = pred_abundance\n)\n\nFor the count predictive performance metrics, we compare observed and estimated counts only for those checklists where the species was detected. In contrast, for the relative abundance metrics we compare observed count and estimate relative abundance for all checklists.\n\n# count metrics\ncount_test <- filter(obs_pred_test, obs_pred_test$obs_count > 0)\ncount_spearman <- cor(count_test$pred_count, \n                      count_test$obs_count,\n                      method = \"spearman\")\nlog_count_pearson <- cor(log(count_test$pred_count + 1),\n                         log(count_test$obs_count + 1),\n                         method = \"pearson\")\n\n# abundance metrics\nabundance_spearman <- cor(obs_pred_test$pred_abundance, \n                          obs_pred_test$obs_count,\n                          method = \"spearman\")\nlog_abundance_pearson <- cor(log(obs_pred_test$pred_abundance + 1),\n                             log(obs_pred_test$obs_count + 1),\n                             method = \"pearson\")\n\n# combine metrics together\nppms <- tibble(\n  count_spearman = count_spearman,\n  log_count_pearson = log_count_pearson,\n  abundance_spearman = abundance_spearman,\n  log_abundance_pearson = log_abundance_pearson\n)\nknitr::kable(pivot_longer(ppms, everything()), digits = 3)\n\n\n\n\nname\nvalue\n\n\n\n\ncount_spearman\n0.233\n\n\nlog_count_pearson\n0.292\n\n\nabundance_spearman\n0.354\n\n\nlog_abundance_pearson\n0.486"
  },
  {
    "objectID": "abundance.html#abundance-predict",
    "href": "abundance.html#abundance-predict",
    "title": "5  Relative Abundance",
    "section": "5.3 Prediction",
    "text": "5.3 Prediction\nJust as we did in the previous chapter for encounter rate, we can estimate relative abundance over our prediction surface. First we estimate encounter rate and count, then we multiply these together to get an estimate of relative abundance. Let’s start by added the effort variables to the prediction grid for a standard eBird checklist at the optimal time of day for detecting Wood Thrush. Recall from the previous chapter that we determined the optimal time of day for detecting Wood Thrush was around 6:15AM.\n\npred_surface_eff <- pred_surface %>% \n  mutate(observation_date = ymd(\"2022-06-15\"),\n         year = year(observation_date),\n         day_of_year = yday(observation_date),\n         # determined as optimal time for detection in previous chapter\n         hours_of_day = 6.2,\n         effort_hours = 2,\n         effort_distance_km = 1,\n         effort_speed_kmph = 0.5,\n         number_observers = 1)\n\nNow we can estimate calibrated encounter rate and count for each point on the prediction surface.\n\n# encounter rate estimate\npred_er <- predict(er_model, data = pred_surface_eff, type = \"response\")\npred_er <- pred_er$predictions[, 2]\n# apply calibration\npred_er_cal <- predict(calibration_model, \n                       data.frame(pred = pred_er), \n                       type = \"response\") %>% \n  as.numeric()\n\n# add predicted encounter rate required for count estimates\npred_surface_eff$predicted_er <- pred_er\n# count estimate\npred_count <- predict(count_model, data = pred_surface_eff, type = \"response\")\npred_count <- pred_count$predictions\n\n# add estimates to prediction surface\npredictions <- bind_cols(pred_surface_eff, \n                         encounter_rate = pred_er_cal,\n                         count = pred_count) %>% \n  select(cell_id, x, y, encounter_rate, count) %>% \n  mutate(encounter_rate = pmin(pmax(encounter_rate, 0), 1))\n\nNext, we add a column for the relative abundance estimate (the product of the encounter rate and count estimates), and convert these estimates to raster format.\n\nr_pred <- predictions %>% \n  # estimate relative abundance\n  mutate(abundance = encounter_rate * count) %>% \n  # convert to spatial features\n  st_as_sf(coords = c(\"x\", \"y\"), crs = crs) %>% \n  select(encounter_rate, count, abundance) %>% \n  # rasterize\n  rasterize(r, field = c(\"encounter_rate\", \"count\", \"abundance\"),\n            fun = \"mean\") %>% \n  setNames(c(\"encounter_rate\", \"count\", \"abundance\"))\n\n# save the raster\n# 3 bands: encounter_rate, count, and relative abundance\nr_pred <- writeRaster(r_pred, \"results/abundance_woothr.tif\", \n                      overwrite = TRUE,\n                      gdal = \"COMPRESS=DEFLATE\")\n\nFinally we’ll produce a map of relative abundance. The values shown on this map are the expected number of Wood Thrush seen by an average eBirder conducting a 1 hour, 1 km checklist for which counting started at about 6:15AM on June 15, 2022. Since detectability is not perfect, we expect true Wood Thrush abundance to be higher than these values, but without estimating the detection rate directly it’s difficult to say how much higher.\n\npar(mar = c(3.5, 0.25, 0.25, 0.25))\n# set up plot area\nplot(study_region, col = NA, border = NA)\nplot(ne_land, col = \"#dddddd\", border = \"#888888\", lwd = 0.5, add = TRUE)\n\n# define quantile breaks\nbrks <- global(r_pred[[\"abundance\"]], fun = quantile, \n               probs = seq(0, 1, 0.1), na.rm = TRUE) %>% \n  as.numeric() %>% \n  unique()\n# label the bottom, middle, and top value\nlbls <- round(c(0, median(brks), max(brks)), 2)\n# ebird status and trends color palette\npal <- abundance_palette(length(brks) - 1)\nplot(r_pred[[\"abundance\"]], \n     col = pal, breaks = brks, \n     maxpixels = ncell(r_pred),\n     legend = FALSE, axes = FALSE, bty = \"n\",\n     add = TRUE)\n\n# borders\nplot(ne_state_lines, col = \"#ffffff\", lwd = 0.75, add = TRUE)\nplot(ne_country_lines, col = \"#ffffff\", lwd = 1.5, add = TRUE)\nplot(study_region, border = \"#000000\", col = NA, lwd = 1, add = TRUE)\nbox()\n\n# legend\npar(new = TRUE, mar = c(0, 0, 0, 0))\ntitle <- \"Wood Thrush Relative Abundance (June 2022)\"\nimage.plot(zlim = c(0, 1), legend.only = TRUE, \n           col = pal, breaks = seq(0, 1, length.out = length(brks)),\n           smallplot = c(0.25, 0.75, 0.06, 0.09),\n           horizontal = TRUE,\n           axis.args = list(at = c(0, 0.5, 1), labels = lbls,\n                            fg = \"black\", col.axis = \"black\",\n                            cex.axis = 0.75, lwd.ticks = 0.5,\n                            padj = -1.5),\n           legend.args = list(text = title,\n                              side = 3, col = \"black\",\n                              cex = 1, line = 0))\n\n\n\n\n\n\n\n\n\n\n\n\nKeyser, Spencer R., Daniel Fink, David Gudex-Cross, Volker C. Radeloff, Jonathan N. Pauli, and Benjamin Zuckerberg. 2023. “Snow Cover Dynamics: An Overlooked yet Important Feature of Winter Bird Occurrence and Abundance Across the United States.” Ecography 2023 (1). https://doi.org/10.1111/ecog.06378."
  },
  {
    "objectID": "references.html",
    "href": "references.html",
    "title": "References",
    "section": "",
    "text": "Amatulli, Giuseppe, Sami Domisch, Mao-Ning Tuanmu, Benoit Parmentier,\nAjay Ranipeta, Jeremy Malczyk, and Walter Jetz. 2018. “A Suite of\nGlobal, Cross-Scale Topographic Variables for Environmental and\nBiodiversity Modeling.” Scientific Data 5 (March):\n180040. https://doi.org/10.1038/sdata.2018.40.\n\n\nCao, Chang, Davide Chicco, and Michael M. Hoffman. 2020. “The\nMCC-F1 Curve: A Performance Evaluation Technique for Binary\nClassification.” https://doi.org/10.48550/ARXIV.2006.11278.\n\n\nChen, Chao, Andy Liaw, and Leo Breiman. 2004. “Using Random Forest\nto Learn Imbalanced Data.” University of California,\nBerkeley 110 (1-12): 24.\n\n\nCourter, Jason R., Ron J. Johnson, Claire M. Stuyck, Brian A. Lang, and\nEvan W. Kaiser. 2013. “Weekend Bias in Citizen\nScience Data Reporting: Implications for Phenology\nStudies.” International Journal of Biometeorology 57\n(5): 715–20. https://doi.org/10.1007/s00484-012-0598-7.\n\n\nEllis, Murray V., and Jennifer E. Taylor. 2018. “Effects of\nWeather, Time of Day, and Survey Effort on Estimates of Species Richness\nin Temperate Woodlands.” Emu-Austral Ornithology 118\n(2): 183–92.\n\n\nFriedl, Mark, and Damien Sulla-Menashe. 2015. “MCD12Q1\nMODIS/Terra+Aqua Land Cover Type Yearly L3\nGlobal 500m SIN Grid V006.” NASA EOSDIS\nLand Processes DAAC. https://doi.org/10.5067/MODIS/MCD12Q1.006.\n\n\nGreenwood, Jeremy J. D. 2007. “Citizens, Science and Bird\nConservation.” Journal of Ornithology 148 (1): 77–124.\nhttps://doi.org/10.1007/s10336-007-0239-9.\n\n\nGuillera-Arroita, Gurutzeta, José J. Lahoz-Monfort, Jane Elith, Ascelin\nGordon, Heini Kujala, Pia E. Lentini, Michael A. McCarthy, Reid Tingley,\nand Brendan A. Wintle. 2015. “Is My Species Distribution Model Fit\nfor Purpose? Matching Data and Models to\nApplications.” Global Ecology and Biogeography 24 (3):\n276–92.\n\n\nJohnston, Alison, Daniel Fink, Wesley M. Hochachka, and Steve Kelling.\n2018. “Estimates of Observer Expertise Improve Species\nDistributions from Citizen Science Data.” Methods in Ecology\nand Evolution 9 (1): 88–97.\n\n\nJohnston, Alison, Stuart E. Newson, Kate Risely, Andy J. Musgrove, Dario\nMassimino, Stephen R. Baillie, and James W. Pearce-Higgins. 2014.\n“Species Traits Explain Variation in Detectability of\nUK Birds.” Bird Study 61 (3): 340–50.\n\n\nKadmon, Ronen, Oren Farber, and Avinoam Danin. 2004. “Effect of\nRoadside Bias on the Accuracy of Predictive Maps Produced by Bioclimatic\nModels.” Ecological Applications 14 (2): 401–13.\n\n\nKelling, Steve, Alison Johnston, Daniel Fink, Viviana Ruiz-Gutierrez,\nRick Bonney, Aletta Bonn, Miguel Fernandez, et al. 2018. “Finding\nthe Signal in the Noise of Citizen Science\nObservations.” bioRxiv, May, 326314. https://doi.org/10.1101/326314.\n\n\nKeyser, Spencer R., Daniel Fink, David Gudex-Cross, Volker C. Radeloff,\nJonathan N. Pauli, and Benjamin Zuckerberg. 2023. “Snow Cover\nDynamics: An Overlooked yet Important Feature of Winter Bird Occurrence\nand Abundance Across the United States.”\nEcography 2023 (1). https://doi.org/10.1111/ecog.06378.\n\n\nLa Sorte, Frank A., Christopher A. Lepczyk, Jessica L. Burnett, Allen H.\nHurlbert, Morgan W. Tingley, and Benjamin Zuckerberg. 2018.\n“Opportunities and Challenges for Big Data Ornithology.”\nThe Condor 120 (2): 414–26.\n\n\nLuck, Gary W., Taylor H. Ricketts, Gretchen C. Daily, and Marc Imhoff.\n2004. “Alleviating Spatial Conflict Between People and\nBiodiversity.” Proceedings of the National Academy of\nSciences 101 (1): 182–86. https://doi.org/10.1073/pnas.2237148100.\n\n\nMurphy, Allan H. 1973. “A New Vector Partition of the\nProbability Score.” Journal of Applied\nMeteorology 12 (4): 595–600. https://doi.org/10.1175/1520-0450(1973)012<0595:ANVPOT>2.0.CO;2.\n\n\nNiculescu-Mizil, Alexandru, and Rich Caruana. 2005. “Predicting\nGood Probabilities with Supervised Learning.” In Proceedings\nof the 22nd International Conference on Machine\nLearning, 625–32. ACM.\n\n\nOliveira, Camilo Viana, Fabio Olmos, Manoel dos Santos-Filho, and\nChristine Steiner São Bernardo. 2018. “Observation of\nDiurnal Soaring Raptors In Northeastern Brazil Depends On Weather\nConditions and Time of Day.”\nJournal of Raptor Research 52 (1): 56–65.\n\n\nPlatt, John. 1999. “Probabilistic Outputs for Support Vector\nMachines and Comparisons to Regularized Likelihood Methods.”\nAdvances in Large Margin Classifiers 10 (3): 61–74.\n\n\nPrendergast, J. R., S. N. Wood, J. H. Lawton, and B. C. Eversham. 1993.\n“Correcting for Variation in Recording Effort in Analyses of\nDiversity Hotspots.” Biodiversity Letters, 39–53.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, and Daniel Fink. 2018.\n“Correcting for Bias in Distribution Modelling for Rare Species\nUsing Citizen Science Data.” Diversity and Distributions\n24 (4): 460–72. https://doi.org/10.1111/ddi.12698.\n\n\nRobinson, Orin J., Viviana Ruiz-Gutierrez, Daniel Fink, Robert J. Meese,\nMarcel Holyoak, and Evan G. Cooch. 2018. “Using Citizen Science\nData in Integrated Population Models to Inform Conservation\nDecision-Making.” bioRxiv, 293464.\n\n\nSullivan, Brian L., Jocelyn L. Aycrigg, Jessie H. Barry, Rick E. Bonney,\nNicholas Bruns, Caren B. Cooper, Theo Damoulas, et al. 2014. “The\neBird Enterprise: An Integrated\nApproach to Development and Application of Citizen Science.”\nBiological Conservation 169 (January): 31–40. https://doi.org/10.1016/j.biocon.2013.11.003.\n\n\nTulloch, Ayesha IT, and Judit K. Szabo. 2012. “A Behavioural\nEcology Approach to Understand Volunteer Surveying for Citizen Science\nDatasets.” Emu-Austral Ornithology 112 (4): 313–25.\n\n\nVaughan, I. P., and S. J. Ormerod. 2005. “The Continuing\nChallenges of Testing Species Distribution Models.” Journal\nof Applied Ecology 42 (4): 720–30. https://doi.org/10.1111/j.1365-2664.2005.01052.x."
  }
]